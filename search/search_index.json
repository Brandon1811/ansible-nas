{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"After getting burned by broken FreeNAS updates one too many times, I figured I could do a much better job myself using just a stock Ubuntu install, some clever Ansible config and a bunch of Docker containers. Ansible-NAS was born! Features An awesome dashboard to your home server (Heimdall) Any number of Samba shares for you to store your stuff A BitTorrent client Various media management tools - Sonarr, Sickchill, CouchPotato, Radarr Media streaming via Plex, Emby or MiniDLNA Music streaming with Airsonic A Dropbox replacement via Nextcloud Various ways to see stats about your NAS - Glances, dashboards in Grafana A backup tool - allows scheduled backups to Amazon S3, OneDrive, Dropbox etc An IRC bouncer Source control with Gitea SSL secured external access to some applications via Traefik A Docker host with Portainer management - run anything that's shipped as a Docker container Getting Started Head to installation if you're ready to roll, or to testing if you want to spin up a test Virtual Machine first. Once you're done, check out the post-installation steps. If this is all very confusing, there is also an overview of the project and what is required for complete beginners. If you're only confused about ZFS, we'll help you get started as well.","title":"Home"},{"location":"#features","text":"An awesome dashboard to your home server (Heimdall) Any number of Samba shares for you to store your stuff A BitTorrent client Various media management tools - Sonarr, Sickchill, CouchPotato, Radarr Media streaming via Plex, Emby or MiniDLNA Music streaming with Airsonic A Dropbox replacement via Nextcloud Various ways to see stats about your NAS - Glances, dashboards in Grafana A backup tool - allows scheduled backups to Amazon S3, OneDrive, Dropbox etc An IRC bouncer Source control with Gitea SSL secured external access to some applications via Traefik A Docker host with Portainer management - run anything that's shipped as a Docker container","title":"Features"},{"location":"#getting-started","text":"Head to installation if you're ready to roll, or to testing if you want to spin up a test Virtual Machine first. Once you're done, check out the post-installation steps. If this is all very confusing, there is also an overview of the project and what is required for complete beginners. If you're only confused about ZFS, we'll help you get started as well.","title":"Getting Started"},{"location":"hardware/","text":"Hardware Ansible-NAS will run against any x64 Ubuntu box (i.e. not a Raspberry Pi or other ARM hardware), or even a VM if you're just testing things out. The hardware you pick for Ansible-NAS depends largely on what you intend to do with your server - and is very much a \"how long is a piece of string\" type question. The homeserver Reddit has lots of good information. ServerBuilds.net is also kept up to date with cheap and decent quality builds known as \"NAS Killers\". Questions To Think About What will be taxing the CPU(s) on your Ansible-NAS box? Are you intending to transcode video? How many users will be hitting it? How many applications do you intend to run in parallel? How much memory do these applications require? Are you going to run the ZFS file system? (you should!) What are you intending to store on your Ansible-NAS? Is it data you can download again, or is it important to you that you don't lose it? Do you need mirrored disks? HP Microserver Ansible-NAS development is tested against an HP Microserver N54L, with 16GB of memory, a 60GB SSD for the OS and 4x2TB WD Red NAS drives for storage. It works great :-) This is obviously not the only solution but a reasonable one if you just want a single box to buy, and many different models are available on eBay for varying costs.","title":"Hardware"},{"location":"hardware/#hardware","text":"Ansible-NAS will run against any x64 Ubuntu box (i.e. not a Raspberry Pi or other ARM hardware), or even a VM if you're just testing things out. The hardware you pick for Ansible-NAS depends largely on what you intend to do with your server - and is very much a \"how long is a piece of string\" type question. The homeserver Reddit has lots of good information. ServerBuilds.net is also kept up to date with cheap and decent quality builds known as \"NAS Killers\".","title":"Hardware"},{"location":"hardware/#questions-to-think-about","text":"What will be taxing the CPU(s) on your Ansible-NAS box? Are you intending to transcode video? How many users will be hitting it? How many applications do you intend to run in parallel? How much memory do these applications require? Are you going to run the ZFS file system? (you should!) What are you intending to store on your Ansible-NAS? Is it data you can download again, or is it important to you that you don't lose it? Do you need mirrored disks?","title":"Questions To Think About"},{"location":"hardware/#hp-microserver","text":"Ansible-NAS development is tested against an HP Microserver N54L, with 16GB of memory, a 60GB SSD for the OS and 4x2TB WD Red NAS drives for storage. It works great :-) This is obviously not the only solution but a reasonable one if you just want a single box to buy, and many different models are available on eBay for varying costs.","title":"HP Microserver"},{"location":"installation/","text":":skull: :skull: :skull: Before running anything, check out the playbook and understand what it does. Run it against a VM and make sure you're happy. Do not blindly download code from the internet and trust that it's going to work as you expect. :skull: :skull: :skull: You can run Ansible-NAS from the computer you plan to use for your NAS, or from a remote controlling machine. The steps for deployment are exactly the same, just pay attention to editing the inventory file in step 7. Enable the Ubuntu Universe repository: sudo add-apt-repository universe Install Ansible: sudo apt update sudo apt install software-properties-common sudo apt-add-repository --yes --update ppa:ansible/ansible sudo apt install ansible Clone Ansible-NAS: git clone https://github.com/davestephens/ansible-nas.git && cd ansible-nas Create your own inventory and config files by copying inventories/sample to your own directory: cp -rfp inventories/sample inventories/my-ansible-nas Review group_vars/all.yml . Change settings by overriding them in inventories/my-ansible-nas/group_vars/nas.yml . Update inventories/my-ansible-nas/inventory . Install the dependent roles: ansible-galaxy install -r requirements.yml (you might need sudo to install Ansible roles). Run the playbook - something like ansible-playbook -i inventories/my-ansible-nas/inventory nas.yml -b -K should do you nicely.","title":"Installation"},{"location":"overview/","text":"Ansible-NAS currently assumes you know your way around a server. This page is an overview for absolute NAS beginners so they can decide if it is right for them. The big picture To start off really simple: A NAS ( Network Attached Storage ) is a server mostly for home or other small networks that offers file storage. It's usually a small box that sits in the corner and runs 24/7. These days, a NAS doesn't just only handle files, but also offers other services, for instance video streaming with Plex or Emby . You can buy consumer NAS boxes from various manufacturers where you just have to add the hard drives, or you can configure your own hardware and use open-source software as the operating system. One example of the second variant you'll see mentioned here is FreeNAS . It is based on FreeBSD , which like Linux belongs to the family of Unix-like operating systems. One strength of FreeBSD/FreeNAS is that it includes the powerful ZFS file system ( OpenZFS , to be exact). However, it does not support the Docker containers the way Linux does. Also, the Linux ecosystem is larger. On the other hand, very few Linux distributions include ZFS out of the box because of licensing issues. Ansible-NAS in its default form attempts to have the best of both worlds by using Docker on Linux with ZFS. This is possible because the Ubuntu Linux distribution supports both technologies. As the name says, Ansible-NAS uses Ansible server automation which is usually deployed on big multi-machine enterprise systems, not small home servers the size of a breadbox. Before you take the plunge The commercial NAS vendors try to make setting up and running a NAS as simple and painless as possible - for a fee, obviously. The open-source NAS software providers have lots of resources to help you get started with your own hardware. FreeNAS for instance comes with extensive documentation, good introductions to ZFS and other topics, and a large community to lean on. With Ansible-NAS, at this point at least, you're pretty much on your own. Though there is a Gitter chat room (see support ), you're expected to have some familiarity with the technologies involved and be able to set up the basic stuff yourself. As a to-do list, before you can even install Ansible-NAS, you'll have to: Choose, buy, configure, and test your own hardware . If you're paranoid (a good mindset when dealing with servers), you'll probably want an uninterruptible power supply (UPS) of some sort as well as SMART monitoring for your hard drives. See the FreeNAS hardware requirements as a guideline, but remember you'll also be running Docker. If you use ZFS (see below), take into account it loves RAM and prefers to have the hard drives all to itself. Install Ubuntu Server , currently 20.04 LTS, and keep it updated. You'll probably want to perform other basic setup tasks like hardening SSH and including email notifications. There are various guides for this, but if you're just getting started, you'll probably need a book. You will probably want to install a specialized filesystem for bulk storage such as ZFS or Btrfs . Both offer features such as snapshots, checksumming and scrubbing to protect your data against bitrot, ransomware and other nasties. Ansible-NAS historically prefers ZFS because this lets you swap storage pools with FreeNAS . A brief introduction to ZFS is included in the Ansible-NAS documentation, as well as an example of a very simple ZFS setup. After that, you can continue with the actual installation of Ansible-NAS. How to experiment The easiest way to take Ansible-NAS for a spin is in a virtual machine, for instance in VirtualBox . You'll want to create three virtual hard drives for testing: One of the actual NAS, and the two others to create a mirrored ZFS pool. This will let you experiment with installing, configuring, and running a complete system.","title":"Overview"},{"location":"overview/#the-big-picture","text":"To start off really simple: A NAS ( Network Attached Storage ) is a server mostly for home or other small networks that offers file storage. It's usually a small box that sits in the corner and runs 24/7. These days, a NAS doesn't just only handle files, but also offers other services, for instance video streaming with Plex or Emby . You can buy consumer NAS boxes from various manufacturers where you just have to add the hard drives, or you can configure your own hardware and use open-source software as the operating system. One example of the second variant you'll see mentioned here is FreeNAS . It is based on FreeBSD , which like Linux belongs to the family of Unix-like operating systems. One strength of FreeBSD/FreeNAS is that it includes the powerful ZFS file system ( OpenZFS , to be exact). However, it does not support the Docker containers the way Linux does. Also, the Linux ecosystem is larger. On the other hand, very few Linux distributions include ZFS out of the box because of licensing issues. Ansible-NAS in its default form attempts to have the best of both worlds by using Docker on Linux with ZFS. This is possible because the Ubuntu Linux distribution supports both technologies. As the name says, Ansible-NAS uses Ansible server automation which is usually deployed on big multi-machine enterprise systems, not small home servers the size of a breadbox.","title":"The big picture"},{"location":"overview/#before-you-take-the-plunge","text":"The commercial NAS vendors try to make setting up and running a NAS as simple and painless as possible - for a fee, obviously. The open-source NAS software providers have lots of resources to help you get started with your own hardware. FreeNAS for instance comes with extensive documentation, good introductions to ZFS and other topics, and a large community to lean on. With Ansible-NAS, at this point at least, you're pretty much on your own. Though there is a Gitter chat room (see support ), you're expected to have some familiarity with the technologies involved and be able to set up the basic stuff yourself. As a to-do list, before you can even install Ansible-NAS, you'll have to: Choose, buy, configure, and test your own hardware . If you're paranoid (a good mindset when dealing with servers), you'll probably want an uninterruptible power supply (UPS) of some sort as well as SMART monitoring for your hard drives. See the FreeNAS hardware requirements as a guideline, but remember you'll also be running Docker. If you use ZFS (see below), take into account it loves RAM and prefers to have the hard drives all to itself. Install Ubuntu Server , currently 20.04 LTS, and keep it updated. You'll probably want to perform other basic setup tasks like hardening SSH and including email notifications. There are various guides for this, but if you're just getting started, you'll probably need a book. You will probably want to install a specialized filesystem for bulk storage such as ZFS or Btrfs . Both offer features such as snapshots, checksumming and scrubbing to protect your data against bitrot, ransomware and other nasties. Ansible-NAS historically prefers ZFS because this lets you swap storage pools with FreeNAS . A brief introduction to ZFS is included in the Ansible-NAS documentation, as well as an example of a very simple ZFS setup. After that, you can continue with the actual installation of Ansible-NAS.","title":"Before you take the plunge"},{"location":"overview/#how-to-experiment","text":"The easiest way to take Ansible-NAS for a spin is in a virtual machine, for instance in VirtualBox . You'll want to create three virtual hard drives for testing: One of the actual NAS, and the two others to create a mirrored ZFS pool. This will let you experiment with installing, configuring, and running a complete system.","title":"How to experiment"},{"location":"post_installation/","text":"So you've installed Ansible-NAS. Now what? The first thing to do is to configure Heimdall as the dashboard of your new NAS, because most of the applications included come with a web interface. Heimdall lets you create \"apps\" for them which appear as little icons on the screen. To add applications to Heimdall, you'll need the IP address of your NAS. If you don't know it for some reason, you will have to look up using the console with ip a . The entry \"link/ether\", usually the second one after the loopback device, will show the address. Another alternative is to make sure Avahi is installed for zero-configuration networking (mDNS). This will allow you to ssh into your NAS and with the extension .local to your machines name, such as ssh tardis.local . Then you can use the ip a command again. Next, you need the application's port, which you can look up in the list of ports . You can test the combination of address and port in your browser by typing them joined by a colon. For instance, for Glances on a machine with the IPv4 address 192.168.1.2, the full address would be http://192.168.1.2:61208 . Once you are sure it works, use this address and port combination when creating the Heimdall icon. Glances and Portainer are probably the two applications you want to add to Heimdall first, so you can see what is happening on the NAS. Note that Portainer will ask for your admin password. After that, it depends on what you have installed - see the listing for individual applications for more information.","title":"Post installation"},{"location":"support/","text":"Support Getting support for Ansible-NAS is easy! Gitter.im Ansible-NAS has its own Gitter chat room. davestephens hangs out there as well as a few existing users. Come say hi! GitHub Issues Raise an issue , using the supplied template to provide as much information as possible.","title":"Support"},{"location":"support/#support","text":"Getting support for Ansible-NAS is easy!","title":"Support"},{"location":"support/#gitterim","text":"Ansible-NAS has its own Gitter chat room. davestephens hangs out there as well as a few existing users. Come say hi!","title":"Gitter.im"},{"location":"support/#github-issues","text":"Raise an issue , using the supplied template to provide as much information as possible.","title":"GitHub Issues"},{"location":"testing/","text":"Vagrant A Vagrant Vagrantfile and launch script ( tests/test-vagrant.sh ) are provided to spin up a testing VM. The config in tests/test.yml is used by the script to override any existing config in group_vars/all.yml . By default the VM will be available on 172.30.1.5. If everything has worked correctly after running tests/test-vagrant.sh , you should be able to connect to Heimdall on http://172.30.1.5:10080. After making changes to the playbook, you can apply them to the running VM by running vagrant provision . Once you're done testing, destroy the VM with vagrant destroy . Travis CI Travis CI runs some sanity checks against branches once pushed to GitHub. These can be viewed here . ansible-lint ansible-lint is run as part of the CI (and VSCode tasks are provided) to ensure the playbook confirms to some sort of standard! You may or may not agree with all of the rules, but using it keeps things nice and consistent. Syntax Checking ansible-playbook --syntax-check is run against nas.yml to ensure nothing is majorly broken.","title":"Testing"},{"location":"testing/#vagrant","text":"A Vagrant Vagrantfile and launch script ( tests/test-vagrant.sh ) are provided to spin up a testing VM. The config in tests/test.yml is used by the script to override any existing config in group_vars/all.yml . By default the VM will be available on 172.30.1.5. If everything has worked correctly after running tests/test-vagrant.sh , you should be able to connect to Heimdall on http://172.30.1.5:10080. After making changes to the playbook, you can apply them to the running VM by running vagrant provision . Once you're done testing, destroy the VM with vagrant destroy .","title":"Vagrant"},{"location":"testing/#travis-ci","text":"Travis CI runs some sanity checks against branches once pushed to GitHub. These can be viewed here .","title":"Travis CI"},{"location":"testing/#ansible-lint","text":"ansible-lint is run as part of the CI (and VSCode tasks are provided) to ensure the playbook confirms to some sort of standard! You may or may not agree with all of the rules, but using it keeps things nice and consistent.","title":"ansible-lint"},{"location":"testing/#syntax-checking","text":"ansible-playbook --syntax-check is run against nas.yml to ensure nothing is majorly broken.","title":"Syntax Checking"},{"location":"upgrading/","text":"Upgrading Ansible-NAS Upgrading from prior to January 2020 ( all.yml.dist config style) If you're upgrading from this commit or earlier, these instructions are relevant to you. Rather than having to merge every new config line into your own all.yml file, now you only need to maintain the differences that are relevant to you in your own nas.yml , stored within an inventory directory. Your inventory nas.yml takes precedence over group_vars/all.yml , which is how this setup works. group_vars/all.yml is now tracked as part of the repo. This will make updates from master much simpler, as there will be no requirement to merge changes from all.yml.dist into your own all.yml any more. You simply pull from master, then add the bits you're interested in into your inventory nas.yml . Instructions to upgrade from prior to January 2020 ( this commit or earlier): Move your group_vars/all.yml somewhere safe. Pull from master. There shouldn't be any merge conflicts unless you've been hacking on the project. Create your own inventory and config files by copying inventories/sample to your own directory: cp -rfp inventories/sample inventories/my-ansible-nas Note that my-ansible-nas can be anything you want, but adjust the following instructions accordingly. Then: Quick and Dirty: Copy the contents of your all.yml into inventories/my-ansible-nas/group_vars/nas.yml . Nice and Tidy: Copy only the differences between your own all.yml and the distribution group_vars/all.yml into inventories/my-ansible-nas/group_vars/nas.yml . This is likely to be things like ansible_nas_hostname , samba_shares , ansible_nas_timezone , enabled applications, any application tweaks you've made in config etc.","title":"Upgrading Ansible-NAS"},{"location":"upgrading/#upgrading-ansible-nas","text":"","title":"Upgrading Ansible-NAS"},{"location":"upgrading/#upgrading-from-prior-to-january-2020-allymldist-config-style","text":"If you're upgrading from this commit or earlier, these instructions are relevant to you. Rather than having to merge every new config line into your own all.yml file, now you only need to maintain the differences that are relevant to you in your own nas.yml , stored within an inventory directory. Your inventory nas.yml takes precedence over group_vars/all.yml , which is how this setup works. group_vars/all.yml is now tracked as part of the repo. This will make updates from master much simpler, as there will be no requirement to merge changes from all.yml.dist into your own all.yml any more. You simply pull from master, then add the bits you're interested in into your inventory nas.yml . Instructions to upgrade from prior to January 2020 ( this commit or earlier): Move your group_vars/all.yml somewhere safe. Pull from master. There shouldn't be any merge conflicts unless you've been hacking on the project. Create your own inventory and config files by copying inventories/sample to your own directory: cp -rfp inventories/sample inventories/my-ansible-nas Note that my-ansible-nas can be anything you want, but adjust the following instructions accordingly. Then: Quick and Dirty: Copy the contents of your all.yml into inventories/my-ansible-nas/group_vars/nas.yml . Nice and Tidy: Copy only the differences between your own all.yml and the distribution group_vars/all.yml into inventories/my-ansible-nas/group_vars/nas.yml . This is likely to be things like ansible_nas_hostname , samba_shares , ansible_nas_timezone , enabled applications, any application tweaks you've made in config etc.","title":"Upgrading from prior to January 2020 (all.yml.dist config style)"},{"location":"applications/airsonic/","text":"Airsonic Homepage: https://airsonic.github.io/ Airsonic is a free, web-based media streamer, providing ubiquitous access to your music. Use it to share your music with friends, or to listen to your own music while at work. You can stream to multiple players simultaneously, for instance to one player in your kitchen and another in your living room Usage Set airsonic_enabled: true in your inventories/<your_inventory>/nas.yml file. The Airsonic web interface can be found at http://ansible_nas_host_or_ip:4040. Specific Configuration The default username and password is admin - you'll need to change this immediately after logging in.","title":"Airsonic"},{"location":"applications/airsonic/#airsonic","text":"Homepage: https://airsonic.github.io/ Airsonic is a free, web-based media streamer, providing ubiquitous access to your music. Use it to share your music with friends, or to listen to your own music while at work. You can stream to multiple players simultaneously, for instance to one player in your kitchen and another in your living room","title":"Airsonic"},{"location":"applications/airsonic/#usage","text":"Set airsonic_enabled: true in your inventories/<your_inventory>/nas.yml file. The Airsonic web interface can be found at http://ansible_nas_host_or_ip:4040.","title":"Usage"},{"location":"applications/airsonic/#specific-configuration","text":"The default username and password is admin - you'll need to change this immediately after logging in.","title":"Specific Configuration"},{"location":"applications/bazarr/","text":"Bazarr subtitle downloader Homepage: https://github.com/morpheus65535/bazarr Bazarr is a companion application to Sonarr and Radarr. It manages and downloads subtitles based on your requirements. You define your preferences by TV show or movie and Bazarr takes care of everything for you. Usage Set bazarr_enabled: true in your inventories/<your_inventory>/nas.yml file. Specific Configuration Follow the Wiki for connecting to Sonarr and Radarr.","title":"Bazarr subtitle downloader"},{"location":"applications/bazarr/#bazarr-subtitle-downloader","text":"Homepage: https://github.com/morpheus65535/bazarr Bazarr is a companion application to Sonarr and Radarr. It manages and downloads subtitles based on your requirements. You define your preferences by TV show or movie and Bazarr takes care of everything for you.","title":"Bazarr subtitle downloader"},{"location":"applications/bazarr/#usage","text":"Set bazarr_enabled: true in your inventories/<your_inventory>/nas.yml file.","title":"Usage"},{"location":"applications/bazarr/#specific-configuration","text":"Follow the Wiki for connecting to Sonarr and Radarr.","title":"Specific Configuration"},{"location":"applications/bitwarden/","text":"Bitwarden(_rs) Password Management Homepage: https://github.com/dani-garcia/bitwarden_rs Bitwarden: https://bitwarden.com/ This is a Bitwarden server API implementation written in Rust compatible with upstream Bitwarden clients*, perfect for self-hosted deployment where running the official resource-heavy service might not be ideal. Usage Set bitwarden_enabled: true in your inventories/<your_inventory>/nas.yml file. Specific Configuration Make sure you set your admin token! It is bitwarden_admin_token in group_vars/all.yml file. The string you put here will be the login to the admin section of your Bitwarden installation (https://bitwarden.ansiblenasdomain.tld/admin). This token can be anything, but it's recommended to use a long, randomly generated string of characters, for example running: openssl rand -base64 48 . To create a user, you need to set bitwarden_allow_signups to true in your all.yml , and re-run the playbook to reprovision the container. Once you've created your users, set bitwarden_allow_signups back to false and run again. For speed you can target just Bitwarden by appending -t bitwarden to your ansible-playbook command.","title":"Bitwarden(_rs) Password Management"},{"location":"applications/bitwarden/#bitwarden_rs-password-management","text":"Homepage: https://github.com/dani-garcia/bitwarden_rs Bitwarden: https://bitwarden.com/ This is a Bitwarden server API implementation written in Rust compatible with upstream Bitwarden clients*, perfect for self-hosted deployment where running the official resource-heavy service might not be ideal.","title":"Bitwarden(_rs) Password Management"},{"location":"applications/bitwarden/#usage","text":"Set bitwarden_enabled: true in your inventories/<your_inventory>/nas.yml file.","title":"Usage"},{"location":"applications/bitwarden/#specific-configuration","text":"Make sure you set your admin token! It is bitwarden_admin_token in group_vars/all.yml file. The string you put here will be the login to the admin section of your Bitwarden installation (https://bitwarden.ansiblenasdomain.tld/admin). This token can be anything, but it's recommended to use a long, randomly generated string of characters, for example running: openssl rand -base64 48 . To create a user, you need to set bitwarden_allow_signups to true in your all.yml , and re-run the playbook to reprovision the container. Once you've created your users, set bitwarden_allow_signups back to false and run again. For speed you can target just Bitwarden by appending -t bitwarden to your ansible-playbook command.","title":"Specific Configuration"},{"location":"applications/calibre/","text":"Calibre-web Homepage: https://github.com/janeczku/calibre-web Calibre-Web is a web app providing a clean interface for browsing, reading and downloading eBooks using an existing Calibre database. Usage Set calibre_enabled: true in your inventories/<your_inventory>/nas.yml file. Specific Configuration Requires Calibre ebook management program. Available for download here . Admin login Default admin login: Username: admin Password: admin123 eBook Conversion If you do not need eBook conversion you can disable it to save resources by setting the calibre_ebook_conversion variable in group_vars/all.yml file to be empty. Conversion enabled: calibre_ebook_conversion: \"linuxserver/calibre-web:calibre\" Conversion disabled: calibre_ebook_conversion: \"\" You can target just Calibre by appending -t calibre to your ansible-playbook command.","title":"Calibre-web"},{"location":"applications/calibre/#calibre-web","text":"Homepage: https://github.com/janeczku/calibre-web Calibre-Web is a web app providing a clean interface for browsing, reading and downloading eBooks using an existing Calibre database.","title":"Calibre-web"},{"location":"applications/calibre/#usage","text":"Set calibre_enabled: true in your inventories/<your_inventory>/nas.yml file.","title":"Usage"},{"location":"applications/calibre/#specific-configuration","text":"Requires Calibre ebook management program. Available for download here .","title":"Specific Configuration"},{"location":"applications/calibre/#admin-login","text":"Default admin login: Username: admin Password: admin123","title":"Admin login"},{"location":"applications/calibre/#ebook-conversion","text":"If you do not need eBook conversion you can disable it to save resources by setting the calibre_ebook_conversion variable in group_vars/all.yml file to be empty. Conversion enabled: calibre_ebook_conversion: \"linuxserver/calibre-web:calibre\" Conversion disabled: calibre_ebook_conversion: \"\" You can target just Calibre by appending -t calibre to your ansible-playbook command.","title":"eBook Conversion"},{"location":"applications/cloudcmd/","text":"Cloud Commander file manager Homepage: https://cloudcmd.io/ Cloud Commander is a file manager for the web. It includes a command-line console and a text editor. Cloud Commander helps you manage your server and work with files, directories and programs in a web browser from any computer, mobile or tablet. Usage Set cloudcmd_enabled: true in your inventories/<your_inventory>/nas.yml file. By default your the root of your Ansible-NAS box ( / ) is mounted into /mnt/fs within the container. If you'd like to change this update cloudcmd_browse_directory in your inventories/<your_inventory>/nas.yml file. If you enable external access to Cloud Commander (note that this is not recommended) then ensure you configure authorisation within the application (F10 from the main menu).","title":"Cloud Commander file manager"},{"location":"applications/cloudcmd/#cloud-commander-file-manager","text":"Homepage: https://cloudcmd.io/ Cloud Commander is a file manager for the web. It includes a command-line console and a text editor. Cloud Commander helps you manage your server and work with files, directories and programs in a web browser from any computer, mobile or tablet.","title":"Cloud Commander file manager"},{"location":"applications/cloudcmd/#usage","text":"Set cloudcmd_enabled: true in your inventories/<your_inventory>/nas.yml file. By default your the root of your Ansible-NAS box ( / ) is mounted into /mnt/fs within the container. If you'd like to change this update cloudcmd_browse_directory in your inventories/<your_inventory>/nas.yml file. If you enable external access to Cloud Commander (note that this is not recommended) then ensure you configure authorisation within the application (F10 from the main menu).","title":"Usage"},{"location":"applications/cloudflare_ddns/","text":"Cloudflare Dynamic DNS Updater Homepage: https://github.com/joshuaavalon/docker-cloudflare Cloudflare: https://www.cloudflare.com If you want your Ansible-NAS accessible externally then you'll need a domain name. You'll also need to set a wildcard host A record to point to your static IP, or enable this container to automatically update Cloudflare with your dynamic IP address. Usage Set cloudflare_ddns_enabled: true in your inventories/<your_inventory>/nas.yml file. Specific Configuration Make sure you set your domain (if different than the ansible-nas default) and access token details within your inventories/<your_inventory>/nas.yml file. If you need to create an API token, see https://joshuaavalon.github.io/docker-cloudflare/guide/cloudflare.html#authentication for instructions. Cloudflare has deprecated global API key authentication. If you have an older ansible-nas configuration using a global API key, you can upgrade to the API token-based authentication by removing the cloudflare_api_key variable from your local nas.yml configuration file and setting the cloudflare_token variable appropriately.","title":"Cloudflare Dynamic DNS Updater"},{"location":"applications/cloudflare_ddns/#cloudflare-dynamic-dns-updater","text":"Homepage: https://github.com/joshuaavalon/docker-cloudflare Cloudflare: https://www.cloudflare.com If you want your Ansible-NAS accessible externally then you'll need a domain name. You'll also need to set a wildcard host A record to point to your static IP, or enable this container to automatically update Cloudflare with your dynamic IP address.","title":"Cloudflare Dynamic DNS Updater"},{"location":"applications/cloudflare_ddns/#usage","text":"Set cloudflare_ddns_enabled: true in your inventories/<your_inventory>/nas.yml file.","title":"Usage"},{"location":"applications/cloudflare_ddns/#specific-configuration","text":"Make sure you set your domain (if different than the ansible-nas default) and access token details within your inventories/<your_inventory>/nas.yml file. If you need to create an API token, see https://joshuaavalon.github.io/docker-cloudflare/guide/cloudflare.html#authentication for instructions. Cloudflare has deprecated global API key authentication. If you have an older ansible-nas configuration using a global API key, you can upgrade to the API token-based authentication by removing the cloudflare_api_key variable from your local nas.yml configuration file and setting the cloudflare_token variable appropriately.","title":"Specific Configuration"},{"location":"applications/couchpotato/","text":"CouchPotato Homepage: https://couchpota.to/ CouchPotato enables you to download movies automatically, easily and in the best quality as soon as they are available. Usage Set couchpotato_enabled: true in your inventories/<your_inventory>/nas.yml file. The CouchPotato web interface can be found at http://ansible_nas_host_or_ip:5050.","title":"CouchPotato"},{"location":"applications/couchpotato/#couchpotato","text":"Homepage: https://couchpota.to/ CouchPotato enables you to download movies automatically, easily and in the best quality as soon as they are available.","title":"CouchPotato"},{"location":"applications/couchpotato/#usage","text":"Set couchpotato_enabled: true in your inventories/<your_inventory>/nas.yml file. The CouchPotato web interface can be found at http://ansible_nas_host_or_ip:5050.","title":"Usage"},{"location":"applications/duplicati/","text":"Duplicati Homepage: https://www.duplicati.com/ Duplicati is free backup software to store encrypted backups online For Windows, macOS and Linux Usage Set duplicati_enabled: true in your inventories/<your_inventory>/nas.yml file. The Duplicati web interface can be found at http://ansible_nas_host_or_ip:8200.","title":"Duplicati"},{"location":"applications/duplicati/#duplicati","text":"Homepage: https://www.duplicati.com/ Duplicati is free backup software to store encrypted backups online For Windows, macOS and Linux","title":"Duplicati"},{"location":"applications/duplicati/#usage","text":"Set duplicati_enabled: true in your inventories/<your_inventory>/nas.yml file. The Duplicati web interface can be found at http://ansible_nas_host_or_ip:8200.","title":"Usage"},{"location":"applications/emby/","text":"Emby Homepage: https://emby.media/ Emby is a mostly open-source media server with a client-server model. This install for Ansible-NAS provides a server, which various clients can then connect to from platforms such as other computers, smartphones and smart TVs. Note that Plex , also included in Ansible-NAS, has a very similar functionality. Usage Set emby_enabled: true in your inventories/<your_inventory>/nas.yml file. There are further parameters you can edit such as movies_root and tv_root lower down. Specific Configuration The emby web interface can be found at port 8096 (http) or 8920 (https, if configured) of your NAS. Heimdall has a dedicated icon for emby. By default, Ansible-NAS gives emby read/write access to the folders where your movies and TV shows are stored. To change this to read-only, edit the following lines in all.yml : emby_movies_permissions: \"rw\" emby_tv_permissions: \"rw\" so that they end in ro instead of rw . Note that emby will not be able to delete files then, which might be exactly what you want. However, you will not have the option to store cover art in the related folders. Always leave the configuration directory read/write. File system considerations Movie and TV show files are almost always very large and pre-compressed. If you are using a specialized filesystem such as ZFS for bulk storage, you'll want to set the parameters accordingly. The ZFS configuration documentation has an example of this. Naming movies and TV shows Emby is very fussy about how movies and TV shows must be named to enable automatic downloads of cover art and metadata. In short, movie files should follow how movies are listed in the IMDb , including the year of publication: movies/Bride of Frankenstein (1935).mp4 Note the spaces. You should probably remove colons and other special characters. TV shows require a folder structure with the name of the series - again if possible with the year of publication - followed by sub-folders for the individual seasons. For example, the first episode of the first season of the original \"Doctor Who\" could be stored as: tv/Doctor Who (1963)/Season 1/Doctor Who - s01e01.mp4 The TVDB is one source for the exact names of TV shows. Unfortunately, there are number of special cases, especially related to split movies and older series. See the movie naming and TV naming guides for further information.","title":"Emby"},{"location":"applications/emby/#emby","text":"Homepage: https://emby.media/ Emby is a mostly open-source media server with a client-server model. This install for Ansible-NAS provides a server, which various clients can then connect to from platforms such as other computers, smartphones and smart TVs. Note that Plex , also included in Ansible-NAS, has a very similar functionality.","title":"Emby"},{"location":"applications/emby/#usage","text":"Set emby_enabled: true in your inventories/<your_inventory>/nas.yml file. There are further parameters you can edit such as movies_root and tv_root lower down.","title":"Usage"},{"location":"applications/emby/#specific-configuration","text":"The emby web interface can be found at port 8096 (http) or 8920 (https, if configured) of your NAS. Heimdall has a dedicated icon for emby. By default, Ansible-NAS gives emby read/write access to the folders where your movies and TV shows are stored. To change this to read-only, edit the following lines in all.yml : emby_movies_permissions: \"rw\" emby_tv_permissions: \"rw\" so that they end in ro instead of rw . Note that emby will not be able to delete files then, which might be exactly what you want. However, you will not have the option to store cover art in the related folders. Always leave the configuration directory read/write.","title":"Specific Configuration"},{"location":"applications/emby/#file-system-considerations","text":"Movie and TV show files are almost always very large and pre-compressed. If you are using a specialized filesystem such as ZFS for bulk storage, you'll want to set the parameters accordingly. The ZFS configuration documentation has an example of this.","title":"File system considerations"},{"location":"applications/emby/#naming-movies-and-tv-shows","text":"Emby is very fussy about how movies and TV shows must be named to enable automatic downloads of cover art and metadata. In short, movie files should follow how movies are listed in the IMDb , including the year of publication: movies/Bride of Frankenstein (1935).mp4 Note the spaces. You should probably remove colons and other special characters. TV shows require a folder structure with the name of the series - again if possible with the year of publication - followed by sub-folders for the individual seasons. For example, the first episode of the first season of the original \"Doctor Who\" could be stored as: tv/Doctor Who (1963)/Season 1/Doctor Who - s01e01.mp4 The TVDB is one source for the exact names of TV shows. Unfortunately, there are number of special cases, especially related to split movies and older series. See the movie naming and TV naming guides for further information.","title":"Naming movies and TV shows"},{"location":"applications/firefly/","text":"Firefly III Homepage: https://firefly-iii.org/ Firefly III is a self-hosted financial manager. It can help you keep track of expenses, income, budgets and everything in between. It supports credit cards, shared household accounts and savings accounts. It\u2019s pretty fancy. You should use it to save and organise money. Usage Set firefly_enabled: true in your inventories/<your_inventory>/nas.yml file. The Firefly III web interface can be found at http://ansible_nas_host_or_ip:8066.","title":"Firefly III"},{"location":"applications/firefly/#firefly-iii","text":"Homepage: https://firefly-iii.org/ Firefly III is a self-hosted financial manager. It can help you keep track of expenses, income, budgets and everything in between. It supports credit cards, shared household accounts and savings accounts. It\u2019s pretty fancy. You should use it to save and organise money.","title":"Firefly III"},{"location":"applications/firefly/#usage","text":"Set firefly_enabled: true in your inventories/<your_inventory>/nas.yml file. The Firefly III web interface can be found at http://ansible_nas_host_or_ip:8066.","title":"Usage"},{"location":"applications/get_iplayer/","text":"get_iplayer Homepage: https://github.com/get-iplayer/get_iplayer Downloads TV and radio programmes from BBC iPlayer. Usage Set get_iplayer_enabled: true in your inventories/<your_inventory>/nas.yml file. The get_iplayer web interface can be found at http://ansible_nas_host_or_ip:8182.","title":"get_iplayer"},{"location":"applications/get_iplayer/#get_iplayer","text":"Homepage: https://github.com/get-iplayer/get_iplayer Downloads TV and radio programmes from BBC iPlayer.","title":"get_iplayer"},{"location":"applications/get_iplayer/#usage","text":"Set get_iplayer_enabled: true in your inventories/<your_inventory>/nas.yml file. The get_iplayer web interface can be found at http://ansible_nas_host_or_ip:8182.","title":"Usage"},{"location":"applications/gitea/","text":"Gitea Homepage: https://gitea.io/ Gitea is a painless self-hosted Git service. Usage Set gitea_enabled: true in your inventories/<your_inventory>/nas.yml file. The Gitea web interface can be found at http://ansible_nas_host_or_ip:3001.","title":"Gitea"},{"location":"applications/gitea/#gitea","text":"Homepage: https://gitea.io/ Gitea is a painless self-hosted Git service.","title":"Gitea"},{"location":"applications/gitea/#usage","text":"Set gitea_enabled: true in your inventories/<your_inventory>/nas.yml file. The Gitea web interface can be found at http://ansible_nas_host_or_ip:3001.","title":"Usage"},{"location":"applications/gitlab/","text":"GitLab Homepage: https://docs.gitlab.com/omnibus/docker/ If Gitea isn't powerful enough for you then consider GitLab. It's a much more powerful (and consequently bigger) Git repository solution that includes a suite of code analytics. On the other hand it requires more RAM. Usage Set gitlab_enabled: true in your inventories/<your_inventory>/nas.yml file. To make GitLab available externally via Traefik set gitlab_available_externally: \"true\" in your inventories/<your_inventory>/nas.yml file. The first time you run GitLab you'll be prompted for an account's password. The password is for GitLab's root administrator account. From there you can log in to create additional users and further configure the application.","title":"GitLab"},{"location":"applications/gitlab/#gitlab","text":"Homepage: https://docs.gitlab.com/omnibus/docker/ If Gitea isn't powerful enough for you then consider GitLab. It's a much more powerful (and consequently bigger) Git repository solution that includes a suite of code analytics. On the other hand it requires more RAM.","title":"GitLab"},{"location":"applications/gitlab/#usage","text":"Set gitlab_enabled: true in your inventories/<your_inventory>/nas.yml file. To make GitLab available externally via Traefik set gitlab_available_externally: \"true\" in your inventories/<your_inventory>/nas.yml file. The first time you run GitLab you'll be prompted for an account's password. The password is for GitLab's root administrator account. From there you can log in to create additional users and further configure the application.","title":"Usage"},{"location":"applications/guacamole/","text":"Guacamole Homepage: hhttps://guacamole.apache.org/ Apache Guacamole is a clientless remote desktop gateway. It supports standard protocols like VNC, RDP, and SSH. Usage Set guacamole_enabled: true in your inventories/<your_inventory>/nas.yml file. Specific Configuration The default username and password is guacadmin . Change it! What to connect to? You can run a virtual desktop from your Ansible-NAS box, check out the Virtual Desktop docs .","title":"Guacamole"},{"location":"applications/guacamole/#guacamole","text":"Homepage: hhttps://guacamole.apache.org/ Apache Guacamole is a clientless remote desktop gateway. It supports standard protocols like VNC, RDP, and SSH.","title":"Guacamole"},{"location":"applications/guacamole/#usage","text":"Set guacamole_enabled: true in your inventories/<your_inventory>/nas.yml file.","title":"Usage"},{"location":"applications/guacamole/#specific-configuration","text":"The default username and password is guacadmin . Change it!","title":"Specific Configuration"},{"location":"applications/guacamole/#what-to-connect-to","text":"You can run a virtual desktop from your Ansible-NAS box, check out the Virtual Desktop docs .","title":"What to connect to?"},{"location":"applications/heimdall/","text":"Heimdall Homepage: https://heimdall.site/ Heimdall Application Dashboard is a dashboard for all your web applications. It doesn't need to be limited to applications though, you can add links to anything you like. There are no iframes here, no apps within apps, no abstraction of APIs. if you think something should work a certain way, it probably does. Usage Set heimdall_enabled: true in your inventories/<your_inventory>/nas.yml file. The Heimdall web interface can be found at http://ansible_nas_host_or_ip:10080.","title":"Heimdall"},{"location":"applications/heimdall/#heimdall","text":"Homepage: https://heimdall.site/ Heimdall Application Dashboard is a dashboard for all your web applications. It doesn't need to be limited to applications though, you can add links to anything you like. There are no iframes here, no apps within apps, no abstraction of APIs. if you think something should work a certain way, it probably does.","title":"Heimdall"},{"location":"applications/heimdall/#usage","text":"Set heimdall_enabled: true in your inventories/<your_inventory>/nas.yml file. The Heimdall web interface can be found at http://ansible_nas_host_or_ip:10080.","title":"Usage"},{"location":"applications/homeassistant/","text":"Home Assistant Homepage: https://www.home-assistant.io/ Open source home automation that puts local control and privacy first. Powered by a worldwide community of tinkerers and DIY enthusiasts. Usage Set homeassistant_enabled: true in your inventories/<your_inventory>/nas.yml file. If you want to access Home Assistant externally, don't forget to set homeassistant_available_externally: \"true\" in your inventories/<your_inventory>/nas.yml file. The Home Assistant web interface can be found at http://ansible_nas_host_or_ip:8123.","title":"Home Assistant"},{"location":"applications/homeassistant/#home-assistant","text":"Homepage: https://www.home-assistant.io/ Open source home automation that puts local control and privacy first. Powered by a worldwide community of tinkerers and DIY enthusiasts.","title":"Home Assistant"},{"location":"applications/homeassistant/#usage","text":"Set homeassistant_enabled: true in your inventories/<your_inventory>/nas.yml file. If you want to access Home Assistant externally, don't forget to set homeassistant_available_externally: \"true\" in your inventories/<your_inventory>/nas.yml file. The Home Assistant web interface can be found at http://ansible_nas_host_or_ip:8123.","title":"Usage"},{"location":"applications/homebridge/","text":"Homebridge Homepage: https://github.com/nfarina/homebridge Homebridge is a lightweight NodeJS server you can run on your home network that emulates the iOS HomeKit API. It supports Plugins, which are community-contributed modules that provide a basic bridge from HomeKit to various 3rd-party APIs provided by manufacturers of \"smart home\" devices. Usage Set homebridge_enabled: true in your inventories/<your_inventory>/nas.yml file. The Homebridge web interface can be found at http://ansible_nas_host_or_ip:8087. The default username and password is 'admin' - change this after your first login!","title":"Homebridge"},{"location":"applications/homebridge/#homebridge","text":"Homepage: https://github.com/nfarina/homebridge Homebridge is a lightweight NodeJS server you can run on your home network that emulates the iOS HomeKit API. It supports Plugins, which are community-contributed modules that provide a basic bridge from HomeKit to various 3rd-party APIs provided by manufacturers of \"smart home\" devices.","title":"Homebridge"},{"location":"applications/homebridge/#usage","text":"Set homebridge_enabled: true in your inventories/<your_inventory>/nas.yml file. The Homebridge web interface can be found at http://ansible_nas_host_or_ip:8087. The default username and password is 'admin' - change this after your first login!","title":"Usage"},{"location":"applications/jackett/","text":"Jackett Homepage: https://github.com/Jackett/Jackett Jackett works as a proxy server: it translates queries from apps (Sonarr, Radarr, SickRage, CouchPotato, Mylar, DuckieTV, qBittorrent, Nefarious etc) into tracker-site-specific http queries, parses the html response, then sends results back to the requesting software. This allows for getting recent uploads (like RSS) and performing searches. Jackett is a single repository of maintained indexer scraping & translation logic - removing the burden from other apps. Usage Set jackett_enabled: true in your inventories/<your_inventory>/nas.yml file. The Jackett web interface can be found at http://ansible_nas_host_or_ip:9117.","title":"Jackett"},{"location":"applications/jackett/#jackett","text":"Homepage: https://github.com/Jackett/Jackett Jackett works as a proxy server: it translates queries from apps (Sonarr, Radarr, SickRage, CouchPotato, Mylar, DuckieTV, qBittorrent, Nefarious etc) into tracker-site-specific http queries, parses the html response, then sends results back to the requesting software. This allows for getting recent uploads (like RSS) and performing searches. Jackett is a single repository of maintained indexer scraping & translation logic - removing the burden from other apps.","title":"Jackett"},{"location":"applications/jackett/#usage","text":"Set jackett_enabled: true in your inventories/<your_inventory>/nas.yml file. The Jackett web interface can be found at http://ansible_nas_host_or_ip:9117.","title":"Usage"},{"location":"applications/jellyfin/","text":"Jellyfin Homepage: https://jellyfin.github.io/ Jellyfin is a Free Software Media System that puts you in control of managing and streaming your media. It is an alternative to the proprietary Emby and Plex, to provide media from a dedicated server to end-user devices via multiple apps. Jellyfin is descended from Emby's 3.5.2 release and ported to the .NET Core framework to enable full cross-platform support. There are no strings attached, no premium licenses or features, and no hidden agendas: just a team who want to build something better and work together to achieve it. We welcome anyone who is interested in joining us in our quest! Note that Plex , also included in Ansible-NAS, has a very similar functionality. Usage Set jellyfin_enabled: true in your inventories/<your_inventory>/nas.yml file. There are further parameters you can edit such as movies_root , tv_root or music_root lower down. Specific Configuration The jellyfin web interface can be found at port 8896 (http) or 8928 (https, if configured) of your NAS. By default, Ansible-NAS gives jellyfin read/write access to the folders where your movies, TV shows and music are stored. To change this to read-only, edit the following lines in all.yml : jellyfin_movies_permissions: \"rw\" jellyfin_tv_permissions: \"rw\" jellyfin_books_permissions: \"rw\" jellyfin_audiobooks_permissions: \"rw\" jellyfin_music_permissions: \"rw\" so that they end in ro instead of rw . Note that jellyfin will not be able to delete files then, which might be exactly what you want. However, you will not have the option to store cover art in the related folders. Always leave the configuration directory read/write. File system considerations Movie and TV show files are almost always very large and pre-compressed. If you are using a specialized filesystem such as ZFS for bulk storage, you'll want to set the parameters accordingly. The ZFS configuration documentation has an example of this. Naming movies and TV shows jellyfin is very fussy about how movies and TV shows must be named to enable automatic downloads of cover art and metadata. In short, movie files should follow how movies are listed in the IMDb , including the year of publication: movies/Bride of Frankenstein (1935).mp4 Note the spaces. You should probably remove colons and other special characters. TV shows require a folder structure with the name of the series - again if possible with the year of publication - followed by sub-folders for the individual seasons. For example, the first episode of the first season of the original \"Doctor Who\" could be stored as: tv/Doctor Who (1963)/Season 1/Doctor Who - s01e01.mp4 The TVDB is one source for the exact names of TV shows. Unfortunately, there are number of special cases, especially related to split movies and older series. See the movie naming and TV naming guides for further information.","title":"Jellyfin"},{"location":"applications/jellyfin/#jellyfin","text":"Homepage: https://jellyfin.github.io/ Jellyfin is a Free Software Media System that puts you in control of managing and streaming your media. It is an alternative to the proprietary Emby and Plex, to provide media from a dedicated server to end-user devices via multiple apps. Jellyfin is descended from Emby's 3.5.2 release and ported to the .NET Core framework to enable full cross-platform support. There are no strings attached, no premium licenses or features, and no hidden agendas: just a team who want to build something better and work together to achieve it. We welcome anyone who is interested in joining us in our quest! Note that Plex , also included in Ansible-NAS, has a very similar functionality.","title":"Jellyfin"},{"location":"applications/jellyfin/#usage","text":"Set jellyfin_enabled: true in your inventories/<your_inventory>/nas.yml file. There are further parameters you can edit such as movies_root , tv_root or music_root lower down.","title":"Usage"},{"location":"applications/jellyfin/#specific-configuration","text":"The jellyfin web interface can be found at port 8896 (http) or 8928 (https, if configured) of your NAS. By default, Ansible-NAS gives jellyfin read/write access to the folders where your movies, TV shows and music are stored. To change this to read-only, edit the following lines in all.yml : jellyfin_movies_permissions: \"rw\" jellyfin_tv_permissions: \"rw\" jellyfin_books_permissions: \"rw\" jellyfin_audiobooks_permissions: \"rw\" jellyfin_music_permissions: \"rw\" so that they end in ro instead of rw . Note that jellyfin will not be able to delete files then, which might be exactly what you want. However, you will not have the option to store cover art in the related folders. Always leave the configuration directory read/write.","title":"Specific Configuration"},{"location":"applications/jellyfin/#file-system-considerations","text":"Movie and TV show files are almost always very large and pre-compressed. If you are using a specialized filesystem such as ZFS for bulk storage, you'll want to set the parameters accordingly. The ZFS configuration documentation has an example of this.","title":"File system considerations"},{"location":"applications/jellyfin/#naming-movies-and-tv-shows","text":"jellyfin is very fussy about how movies and TV shows must be named to enable automatic downloads of cover art and metadata. In short, movie files should follow how movies are listed in the IMDb , including the year of publication: movies/Bride of Frankenstein (1935).mp4 Note the spaces. You should probably remove colons and other special characters. TV shows require a folder structure with the name of the series - again if possible with the year of publication - followed by sub-folders for the individual seasons. For example, the first episode of the first season of the original \"Doctor Who\" could be stored as: tv/Doctor Who (1963)/Season 1/Doctor Who - s01e01.mp4 The TVDB is one source for the exact names of TV shows. Unfortunately, there are number of special cases, especially related to split movies and older series. See the movie naming and TV naming guides for further information.","title":"Naming movies and TV shows"},{"location":"applications/joomla/","text":"Joomla CMS Homepage: https://www.joomla.org/ Joomla! is an award-winning content management system (CMS), which enables you to build web sites and powerful online applications. Usage Set joomla_enabled: true in your inventories/<your_inventory>/nas.yml file. If you want to access Joomla externally, set joomla_available_externally: \"true\" in your inventories/<your_inventory>/nas.yml file. The Joomla web interface can be found at http://ansible_nas_host_or_ip:8181. Specific Configuration Set joomla_database_password in your all.yml before installing Joomla. On first run you'll need to enter database details: Host: mysql Database: joomla Username: root Password: whatever you set for joomla_database_password .","title":"Joomla CMS"},{"location":"applications/joomla/#joomla-cms","text":"Homepage: https://www.joomla.org/ Joomla! is an award-winning content management system (CMS), which enables you to build web sites and powerful online applications.","title":"Joomla CMS"},{"location":"applications/joomla/#usage","text":"Set joomla_enabled: true in your inventories/<your_inventory>/nas.yml file. If you want to access Joomla externally, set joomla_available_externally: \"true\" in your inventories/<your_inventory>/nas.yml file. The Joomla web interface can be found at http://ansible_nas_host_or_ip:8181.","title":"Usage"},{"location":"applications/joomla/#specific-configuration","text":"Set joomla_database_password in your all.yml before installing Joomla. On first run you'll need to enter database details: Host: mysql Database: joomla Username: root Password: whatever you set for joomla_database_password .","title":"Specific Configuration"},{"location":"applications/krusader/","text":"Krusader Homepage: https://krusader.org/ Docker Container: Krusader Krusader provides twin panel file management for your ansible-nas via browser and VNC. Usage Set krusader_enabled: true in your inventories/<your_inventory>/nas.yml file. The Krusader web interface can be found at http://ansible_nas_host_or_ip:5800.","title":"Krusader"},{"location":"applications/krusader/#krusader","text":"Homepage: https://krusader.org/ Docker Container: Krusader Krusader provides twin panel file management for your ansible-nas via browser and VNC.","title":"Krusader"},{"location":"applications/krusader/#usage","text":"Set krusader_enabled: true in your inventories/<your_inventory>/nas.yml file. The Krusader web interface can be found at http://ansible_nas_host_or_ip:5800.","title":"Usage"},{"location":"applications/lidarr/","text":"Lidarr music collection manager Homepage: https://lidarr.audio/ Lidarr is a music collection manager for Usenet and BitTorrent users. It can monitor multiple RSS feeds for new tracks from your favorite artists and will grab, sort and rename them. It can also be configured to automatically upgrade the quality of files already downloaded when a better quality format becomes available. Usage Set lidarr_enabled: true in your inventories/<your_inventory>/nas.yml file.","title":"Lidarr music collection manager"},{"location":"applications/lidarr/#lidarr-music-collection-manager","text":"Homepage: https://lidarr.audio/ Lidarr is a music collection manager for Usenet and BitTorrent users. It can monitor multiple RSS feeds for new tracks from your favorite artists and will grab, sort and rename them. It can also be configured to automatically upgrade the quality of files already downloaded when a better quality format becomes available.","title":"Lidarr music collection manager"},{"location":"applications/lidarr/#usage","text":"Set lidarr_enabled: true in your inventories/<your_inventory>/nas.yml file.","title":"Usage"},{"location":"applications/minidlna/","text":"MiniDLNA Homepage: https://sourceforge.net/projects/minidlna/ MiniDLNA is server software with the aim of being fully compliant with DLNA/UPnP clients. The MiniDNLA daemon serves media files (music, pictures, and video) to clients on a network. Example clients include applications such as Totem and Kodi, and devices such as portable media players, Smartphones, Televisions, and gaming systems (such as PS3 and Xbox 360). Usage Set minidlna_enabled: true in your inventories/<your_inventory>/nas.yml file. The very basic MiniDLNA web interface can be found at http://ansible_nas_host_or_ip:8201.","title":"MiniDLNA"},{"location":"applications/minidlna/#minidlna","text":"Homepage: https://sourceforge.net/projects/minidlna/ MiniDLNA is server software with the aim of being fully compliant with DLNA/UPnP clients. The MiniDNLA daemon serves media files (music, pictures, and video) to clients on a network. Example clients include applications such as Totem and Kodi, and devices such as portable media players, Smartphones, Televisions, and gaming systems (such as PS3 and Xbox 360).","title":"MiniDLNA"},{"location":"applications/minidlna/#usage","text":"Set minidlna_enabled: true in your inventories/<your_inventory>/nas.yml file. The very basic MiniDLNA web interface can be found at http://ansible_nas_host_or_ip:8201.","title":"Usage"},{"location":"applications/miniflux/","text":"Miniflux Homepage: https://miniflux.app/ Miniflux is a minimalist and opinionated feed reader. Usage Set miniflux_enabled: true in your inventories/<your_inventory>/nas.yml file. The Miniflux web interface can be found at http://ansible_nas_host_or_ip:8070, the default username is admin and password supersecure . Specific Configuration An admin user will be created with the username and password of miniflux_admin_username and miniflux_admin_password respectively. These can be found in the Miniflux section within all.yml.dist .","title":"Miniflux"},{"location":"applications/miniflux/#miniflux","text":"Homepage: https://miniflux.app/ Miniflux is a minimalist and opinionated feed reader.","title":"Miniflux"},{"location":"applications/miniflux/#usage","text":"Set miniflux_enabled: true in your inventories/<your_inventory>/nas.yml file. The Miniflux web interface can be found at http://ansible_nas_host_or_ip:8070, the default username is admin and password supersecure .","title":"Usage"},{"location":"applications/miniflux/#specific-configuration","text":"An admin user will be created with the username and password of miniflux_admin_username and miniflux_admin_password respectively. These can be found in the Miniflux section within all.yml.dist .","title":"Specific Configuration"},{"location":"applications/mosquitto/","text":"Mosquitto Homepage: https://mosquitto.org Mosquitto is a lightweight open source MQTT message broker. Usage Set mosquitto_enabled: true in your inventories/<your_inventory>/nas.yml file.","title":"Mosquitto"},{"location":"applications/mosquitto/#mosquitto","text":"Homepage: https://mosquitto.org Mosquitto is a lightweight open source MQTT message broker.","title":"Mosquitto"},{"location":"applications/mosquitto/#usage","text":"Set mosquitto_enabled: true in your inventories/<your_inventory>/nas.yml file.","title":"Usage"},{"location":"applications/mylar/","text":"Mylar Homepage: https://github.com/evilhero/mylar Docker Container: https://hub.docker.com/r/linuxserver/mylar An automated Comic Book downloader (cbr/cbz) for use with SABnzbd, NZBGet and torrents Usage Set mylar_enabled: true in your inventories/<your_inventory>/nas.yml file. If you want to access Mylar externally, don't forget to set mylar_available_externally: \"true\" in your inventories/<your_inventory>/nas.yml file. The Mylar web interface can be found at http://ansible_nas_host_or_ip:5858.","title":"Mylar"},{"location":"applications/mylar/#mylar","text":"Homepage: https://github.com/evilhero/mylar Docker Container: https://hub.docker.com/r/linuxserver/mylar An automated Comic Book downloader (cbr/cbz) for use with SABnzbd, NZBGet and torrents","title":"Mylar"},{"location":"applications/mylar/#usage","text":"Set mylar_enabled: true in your inventories/<your_inventory>/nas.yml file. If you want to access Mylar externally, don't forget to set mylar_available_externally: \"true\" in your inventories/<your_inventory>/nas.yml file. The Mylar web interface can be found at http://ansible_nas_host_or_ip:5858.","title":"Usage"},{"location":"applications/mymediaforalexa/","text":"My Media for Alexa Homepage: https://www.mymediaalexa.com/ My Media lets you stream your music collection to your Amazon Echo or Amazon Dot without having to upload all your music collection to the Cloud. This keeps your music under your control. Usage Set mymediaforalexa_enabled: true in your inventories/<your_inventory>/nas.yml file. The My Media for Alexa web interface can be found at http://ansible_nas_host_or_ip:52051.","title":"My Media for Alexa"},{"location":"applications/mymediaforalexa/#my-media-for-alexa","text":"Homepage: https://www.mymediaalexa.com/ My Media lets you stream your music collection to your Amazon Echo or Amazon Dot without having to upload all your music collection to the Cloud. This keeps your music under your control.","title":"My Media for Alexa"},{"location":"applications/mymediaforalexa/#usage","text":"Set mymediaforalexa_enabled: true in your inventories/<your_inventory>/nas.yml file. The My Media for Alexa web interface can be found at http://ansible_nas_host_or_ip:52051.","title":"Usage"},{"location":"applications/nextcloud/","text":"Nextcloud Homepage: https://nextcloud.com Usage Set nextcloud_enabled: true in your inventories/<your_inventory>/nas.yml file. Tread carefully. External access may require that you manually configure your Fully Qualified Domain Name (FQDN) as a trusted domain within the application. There is an environment variable set up for this in the \"nextcloud task\" which will most likely make manual configuration unnecessary. If you get the following screenshot warning when trying to access nextcloud externally you'll need to manually set it up. This can be accomplished in two commands. # On the server where the docker containers are running $ docker exec -it --user www-data nextcloud /bin/bash $ php occ config:system:set trusted_domains INDEX_FOR_NEW_ENTRY_SEE_DOCS_LINK_BELOW --value=YOUR_FQDN_HERE --update-only The above commands are documented in the administration guide for Nextcloud: set array values docker container docs, references environment variables","title":"Nextcloud"},{"location":"applications/nextcloud/#nextcloud","text":"Homepage: https://nextcloud.com","title":"Nextcloud"},{"location":"applications/nextcloud/#usage","text":"Set nextcloud_enabled: true in your inventories/<your_inventory>/nas.yml file. Tread carefully. External access may require that you manually configure your Fully Qualified Domain Name (FQDN) as a trusted domain within the application. There is an environment variable set up for this in the \"nextcloud task\" which will most likely make manual configuration unnecessary. If you get the following screenshot warning when trying to access nextcloud externally you'll need to manually set it up. This can be accomplished in two commands. # On the server where the docker containers are running $ docker exec -it --user www-data nextcloud /bin/bash $ php occ config:system:set trusted_domains INDEX_FOR_NEW_ENTRY_SEE_DOCS_LINK_BELOW --value=YOUR_FQDN_HERE --update-only The above commands are documented in the administration guide for Nextcloud: set array values docker container docs, references environment variables","title":"Usage"},{"location":"applications/nzbget/","text":"NZBget Homepage: https://nzbget.net/ The most efficient Usenet downloader. NZBGet is written in C++ and designed with performance in mind to achieve maximum download speed by using very little system resources. Usage Set nzbget_enabled: true in your inventories/<your_inventory>/nas.yml file. The NZBget web interface can be found at http://ansible_nas_host_or_ip:6789, the default username is nzbget and password tegbzn6789 . Change this once you've logged in!","title":"NZBget"},{"location":"applications/nzbget/#nzbget","text":"Homepage: https://nzbget.net/ The most efficient Usenet downloader. NZBGet is written in C++ and designed with performance in mind to achieve maximum download speed by using very little system resources.","title":"NZBget"},{"location":"applications/nzbget/#usage","text":"Set nzbget_enabled: true in your inventories/<your_inventory>/nas.yml file. The NZBget web interface can be found at http://ansible_nas_host_or_ip:6789, the default username is nzbget and password tegbzn6789 . Change this once you've logged in!","title":"Usage"},{"location":"applications/ombi/","text":"Ombi Homepage: https://ombi.io/ Ombi is a self-hosted web application that automatically gives your shared Plex or Emby users the ability to request content by themselves! Ombi can be linked to multiple TV Show and Movie DVR tools to create a seamless end-to-end experience for your users. Usage Set ombi_enabled: true in your inventories/<your_inventory>/nas.yml file.","title":"Ombi"},{"location":"applications/ombi/#ombi","text":"Homepage: https://ombi.io/ Ombi is a self-hosted web application that automatically gives your shared Plex or Emby users the ability to request content by themselves! Ombi can be linked to multiple TV Show and Movie DVR tools to create a seamless end-to-end experience for your users.","title":"Ombi"},{"location":"applications/ombi/#usage","text":"Set ombi_enabled: true in your inventories/<your_inventory>/nas.yml file.","title":"Usage"},{"location":"applications/openhab/","text":"openHAB Homepage: https://www.openhab.org/ OpenHab is a vendor and technology agnostic open source automation software for your home. It allows you to connect many different IoT-Devices (which in this case means \"Intranet of Things\") using custom bindings made by the community. Usage Set openhab_enabled: true in your inventories/<your_inventory>/nas.yml file. Specific Configuration The openHAB Webinterface is available at port 7777 (HTTP) and 7778 (HTTPS). Visit the webinterface and follow the setup instructions found in the openHAB Documentation","title":"openHAB"},{"location":"applications/openhab/#openhab","text":"Homepage: https://www.openhab.org/ OpenHab is a vendor and technology agnostic open source automation software for your home. It allows you to connect many different IoT-Devices (which in this case means \"Intranet of Things\") using custom bindings made by the community.","title":"openHAB"},{"location":"applications/openhab/#usage","text":"Set openhab_enabled: true in your inventories/<your_inventory>/nas.yml file.","title":"Usage"},{"location":"applications/openhab/#specific-configuration","text":"The openHAB Webinterface is available at port 7777 (HTTP) and 7778 (HTTPS). Visit the webinterface and follow the setup instructions found in the openHAB Documentation","title":"Specific Configuration"},{"location":"applications/organizr/","text":"Organizr Homepage: https://organizr.app/ ORGANIZR aims to be your one stop shop for your Servers Frontend. Do you have quite a bit of services running on your computer or server? Do you have a lot of bookmarks or have to memor$ Usage Set organizr_enabled: true in your inventories/<your_inventory>/nas.yml file. The Organizr web interface can be found at http://ansible_nas_host_or_ip:10081.","title":"Organizr"},{"location":"applications/organizr/#organizr","text":"Homepage: https://organizr.app/ ORGANIZR aims to be your one stop shop for your Servers Frontend. Do you have quite a bit of services running on your computer or server? Do you have a lot of bookmarks or have to memor$","title":"Organizr"},{"location":"applications/organizr/#usage","text":"Set organizr_enabled: true in your inventories/<your_inventory>/nas.yml file. The Organizr web interface can be found at http://ansible_nas_host_or_ip:10081.","title":"Usage"},{"location":"applications/pyload/","text":"pyLoad Homepage: https://pyload.net/ Free and Open Source download manager written in Python and designed to be extremely lightweight, easily extensible and fully manageable via web . Usage Set pyload_enabled: true in your inventories/<your_inventory>/nas.yml file. pyLoad's web interface can be found at http://ansible_nas_host_or_ip:8000 Specific Configuration Default username is pyload and default password is pyload . In order to add or remove users, you will need to access the container from an interactive shell (can easily be done from portainer, if installed) and enter pyLoad's home directory /opt/pyload and using the command python pyLoadCore.py -u and follow the on-screen prompts. More commands to configure and customize pyLoad can be found on it's website.","title":"pyLoad"},{"location":"applications/pyload/#pyload","text":"Homepage: https://pyload.net/ Free and Open Source download manager written in Python and designed to be extremely lightweight, easily extensible and fully manageable via web .","title":"pyLoad"},{"location":"applications/pyload/#usage","text":"Set pyload_enabled: true in your inventories/<your_inventory>/nas.yml file. pyLoad's web interface can be found at http://ansible_nas_host_or_ip:8000","title":"Usage"},{"location":"applications/pyload/#specific-configuration","text":"Default username is pyload and default password is pyload . In order to add or remove users, you will need to access the container from an interactive shell (can easily be done from portainer, if installed) and enter pyLoad's home directory /opt/pyload and using the command python pyLoadCore.py -u and follow the on-screen prompts. More commands to configure and customize pyLoad can be found on it's website.","title":"Specific Configuration"},{"location":"applications/pytivo/","text":"PyTivo Project Homepage: https://github.com/lucasnz/pytivo Docker Homepage: https://hub.docker.com/r/pinion/docker-pytivo PyTivo is both an HMO and GoBack server. Similar to TiVo Desktop pyTivo loads many standard video compression codecs and outputs mpeg2 video to the TiVo. However, pyTivo is able to load MANY more file types than TiVo Desktop. http://pytivo.org/ Usage Set pytivo_enabled: true in your group_vars/all.yml file. The PyTivo web interface can be found at http://ansible_nas_host_or_ip:9032. Specific Configuration PyTivo needs to be configured for use. Your ansible-nas media is available to share via: * /movies - Where your movies are stored * /music - Where your music is stored * /photos - Where your photos are stored * /podcasts - Where your podcasts are stored * /tv - Where your TV episodes are stored Configuration help for PyTivo settings can be found at Configure_pyTivo .","title":"PyTivo"},{"location":"applications/pytivo/#pytivo","text":"Project Homepage: https://github.com/lucasnz/pytivo Docker Homepage: https://hub.docker.com/r/pinion/docker-pytivo PyTivo is both an HMO and GoBack server. Similar to TiVo Desktop pyTivo loads many standard video compression codecs and outputs mpeg2 video to the TiVo. However, pyTivo is able to load MANY more file types than TiVo Desktop. http://pytivo.org/","title":"PyTivo"},{"location":"applications/pytivo/#usage","text":"Set pytivo_enabled: true in your group_vars/all.yml file. The PyTivo web interface can be found at http://ansible_nas_host_or_ip:9032.","title":"Usage"},{"location":"applications/pytivo/#specific-configuration","text":"PyTivo needs to be configured for use. Your ansible-nas media is available to share via: * /movies - Where your movies are stored * /music - Where your music is stored * /photos - Where your photos are stored * /podcasts - Where your podcasts are stored * /tv - Where your TV episodes are stored Configuration help for PyTivo settings can be found at Configure_pyTivo .","title":"Specific Configuration"},{"location":"applications/radarr/","text":"Radarr Homepage: radarr Radarr is an independent fork of Sonarr reworked for automatically downloading movies via Usenet and BitTorrent. Usage Set radarr_enabled: true in your /inventories/[my inventory]/group_vars/nas.yml file. The Radarr web interface can be found at http://ansible_nas_host_or_ip:7878 by default Specific Configuration First make sure Radarr has permissions to write and read the /download , and /movies folders . Do this by ensuring the radarr_movies_directory: and radarr_download_directory settings are correct. Radarr will get the file path from the Download client. On default settings with Transmission the path is /storage/downloads/complete . You will need to create a path mapping in the Remote Path Mappings settings under Download Client to point to your internal path that is by default /downloads . If you have difficulties with the path mapping you could also just add a new volume path for the Transmission container and use /downloads as the download directory. For Radarr to understand that the /movies folder is a folder, you'll need to add a new subfolder into it. You can also do this by adding a random movie to the folder. Keep in mind to have the internal setting Create empty movie folders on yes Comprehensive setup information can be found on the Radarr GitHub wiki","title":"Radarr"},{"location":"applications/radarr/#radarr","text":"Homepage: radarr Radarr is an independent fork of Sonarr reworked for automatically downloading movies via Usenet and BitTorrent.","title":"Radarr"},{"location":"applications/radarr/#usage","text":"Set radarr_enabled: true in your /inventories/[my inventory]/group_vars/nas.yml file. The Radarr web interface can be found at http://ansible_nas_host_or_ip:7878 by default","title":"Usage"},{"location":"applications/radarr/#specific-configuration","text":"First make sure Radarr has permissions to write and read the /download , and /movies folders . Do this by ensuring the radarr_movies_directory: and radarr_download_directory settings are correct. Radarr will get the file path from the Download client. On default settings with Transmission the path is /storage/downloads/complete . You will need to create a path mapping in the Remote Path Mappings settings under Download Client to point to your internal path that is by default /downloads . If you have difficulties with the path mapping you could also just add a new volume path for the Transmission container and use /downloads as the download directory. For Radarr to understand that the /movies folder is a folder, you'll need to add a new subfolder into it. You can also do this by adding a random movie to the folder. Keep in mind to have the internal setting Create empty movie folders on yes Comprehensive setup information can be found on the Radarr GitHub wiki","title":"Specific Configuration"},{"location":"applications/sonarr/","text":"Sonarr Homepages: sonarr Sonarr is a PVR for Usenet and BitTorrent users. It can monitor multiple RSS feeds for new episodes of your favorite shows and will grab, sort and rename them. It can also be configured to automatically upgrade the quality of files already downloaded when a better quality format becomes available. Usage Set sonarr_enabled: true in your /inventories/[my inventory]/group_vars/nas.yml file. The Sonarr web interface can be found at http://ansible_nas_host_or_ip:8989 by default Specific Configuration First make sure Sonarr has permissions to write and read the /download and /tv folders . Do this by ensuring the sonarr_movies_directory: and sonarr_download_directory settings are correct. Sonarr will get the file path from the Download client. On default settings with Transmission the path is /storage/downloads/complete . You will need to create a path mapping in the Remote Path Mappings settings under Download Client to point to your internal path that is by default /downloads . If you have difficulties with the path mapping you could also just add a new volume path for the Transmission container and use /downloads as the download directory. For Sonarr to understand that the /tv folder is a folder, you'll need to add a folder into it. You can also do this by adding a random series to the folder. Keep in mind to have the setting Create empty movie folders on yes For comprehensive configuration instructions see the Sonarr GitHub wiki","title":"Sonarr"},{"location":"applications/sonarr/#sonarr","text":"Homepages: sonarr Sonarr is a PVR for Usenet and BitTorrent users. It can monitor multiple RSS feeds for new episodes of your favorite shows and will grab, sort and rename them. It can also be configured to automatically upgrade the quality of files already downloaded when a better quality format becomes available.","title":"Sonarr"},{"location":"applications/sonarr/#usage","text":"Set sonarr_enabled: true in your /inventories/[my inventory]/group_vars/nas.yml file. The Sonarr web interface can be found at http://ansible_nas_host_or_ip:8989 by default","title":"Usage"},{"location":"applications/sonarr/#specific-configuration","text":"First make sure Sonarr has permissions to write and read the /download and /tv folders . Do this by ensuring the sonarr_movies_directory: and sonarr_download_directory settings are correct. Sonarr will get the file path from the Download client. On default settings with Transmission the path is /storage/downloads/complete . You will need to create a path mapping in the Remote Path Mappings settings under Download Client to point to your internal path that is by default /downloads . If you have difficulties with the path mapping you could also just add a new volume path for the Transmission container and use /downloads as the download directory. For Sonarr to understand that the /tv folder is a folder, you'll need to add a folder into it. You can also do this by adding a random series to the folder. Keep in mind to have the setting Create empty movie folders on yes For comprehensive configuration instructions see the Sonarr GitHub wiki","title":"Specific Configuration"},{"location":"applications/syncthing/","text":"Syncthing: Open Source Continuous File Synchronisation Homepage: https://syncthing.net/ Github: https://github.com/syncthing/syncthing Docker: https://hub.docker.com/r/syncthing/syncthing Syncthing is a continuous file synchronization program. It synchronizes files between two or more computers. It strives to fulfill the goals below in summary. Syncthing should be: Safe From Data Loss Secure Against Attackers Easy to Use Automatic Universally Available For Individuals For eveything else see the goals document Usage Set syncthing_enabled: true in your \\inventories\\[my inventory]\\group_vars\\nas.yml file. Specific Configuration Open the web interface at :8384 to configure.","title":"Syncthing: Open Source Continuous File Synchronisation"},{"location":"applications/syncthing/#syncthing-open-source-continuous-file-synchronisation","text":"Homepage: https://syncthing.net/ Github: https://github.com/syncthing/syncthing Docker: https://hub.docker.com/r/syncthing/syncthing Syncthing is a continuous file synchronization program. It synchronizes files between two or more computers. It strives to fulfill the goals below in summary. Syncthing should be: Safe From Data Loss Secure Against Attackers Easy to Use Automatic Universally Available For Individuals For eveything else see the goals document","title":"Syncthing: Open Source Continuous File Synchronisation"},{"location":"applications/syncthing/#usage","text":"Set syncthing_enabled: true in your \\inventories\\[my inventory]\\group_vars\\nas.yml file.","title":"Usage"},{"location":"applications/syncthing/#specific-configuration","text":"Open the web interface at :8384 to configure.","title":"Specific Configuration"},{"location":"applications/tautulli/","text":"Tautulli Homepage: https://tautulli.com/ Tautulli allows you to monitor your Plex Media Server. Usage Set tautulli_enabled: true in your inventories/<your_inventory>/nas.yml file. The Tautulli web interface can be found at http://ansible_nas_host_or_ip:8181.","title":"Tautulli"},{"location":"applications/tautulli/#tautulli","text":"Homepage: https://tautulli.com/ Tautulli allows you to monitor your Plex Media Server.","title":"Tautulli"},{"location":"applications/tautulli/#usage","text":"Set tautulli_enabled: true in your inventories/<your_inventory>/nas.yml file. The Tautulli web interface can be found at http://ansible_nas_host_or_ip:8181.","title":"Usage"},{"location":"applications/thelounge/","text":"The Lounge Homepage: https://thelounge.chat/ The Lounge is a self-hosted web IRC client. Usage Set thelounge_enabled: true in your inventories/<your_inventory>/nas.yml file. The Lounge web interface can be found at http://ansible_nas_host_or_ip:9000. Specific Configuration The default username and password is admin . Change this once you've logged in!","title":"The Lounge"},{"location":"applications/thelounge/#the-lounge","text":"Homepage: https://thelounge.chat/ The Lounge is a self-hosted web IRC client.","title":"The Lounge"},{"location":"applications/thelounge/#usage","text":"Set thelounge_enabled: true in your inventories/<your_inventory>/nas.yml file. The Lounge web interface can be found at http://ansible_nas_host_or_ip:9000.","title":"Usage"},{"location":"applications/thelounge/#specific-configuration","text":"The default username and password is admin . Change this once you've logged in!","title":"Specific Configuration"},{"location":"applications/timemachine/","text":"Time Machine Apple docs: https://support.apple.com/en-us/HT201250 Docker image: https://github.com/awlx/samba-timemachine Time Machine is an application that allows you to backup files from your Mac. Older versions of Time Machine relied on AFP (netatalk) shares. Apple has deprecated Time Machine over AFP in favor of SMB (Samba), and current versions of Ansible-NAS use a Samba-based Time Machine share. If you are upgrading from an older version of Ansible-NAS, you will need to re-select your Time Machine back up disk by opening Time Machine Preferences and Selecting your backup disk via the \"Select Disk...\" option. Your Mac will find the old backups on the share and use them. Usage Set timemachine_enabled: true in your inventories/<your_inventory>/nas.yml file. Enabling Time Machine will result in the installation of Avahi on the NAS system (if it is not already installed) and a Time Machine service configuration file for Avahi will be added to the system (at /etc/avahi/services/timemachine.service ) to allow for Time Machine discovery by Macs on the local network. Avahi runs on the system, rather than in a container, as the same Avahi instance can be used to announce any number of services. The Samba server included in the Time Machine docker container logs to STDOUT and is compatible with Docker's built-in logging infrastructure. Specific Configuration timemachine_data_directory The absolute path on Ansible NAS where the backup files will be stored timemachine_volume_size_limit The maximum amount of space Time Machine can use for the backups in units of MiB. Set it to 0 for no limit. timemachine_share_name The name of the share as it will appear in the Time Machine application. Default is 'Data' timemachine_password The password used to access the share. Default is 'timemachine' ## Upgrading from AFP to SMB-based Time Machine Older versions of Time Machine included in Ansible-NAS relied on AFP (netatalk) shares. Apple has deprecated Time Machine over AFP in favor of SMB (Samba), and current versions of Ansible-NAS use a Samba-based Time Machine share. If you are upgrading from an older version of Ansible-NAS with the AFP-based Time Machine, you will need to re-select your Time Machine back up disk by opening Time Machine Preferences and Selecting your backup disk via the \"Select Disk...\" option. Your Mac will find the old backups on the share and use them.","title":"Time Machine"},{"location":"applications/timemachine/#time-machine","text":"Apple docs: https://support.apple.com/en-us/HT201250 Docker image: https://github.com/awlx/samba-timemachine Time Machine is an application that allows you to backup files from your Mac. Older versions of Time Machine relied on AFP (netatalk) shares. Apple has deprecated Time Machine over AFP in favor of SMB (Samba), and current versions of Ansible-NAS use a Samba-based Time Machine share. If you are upgrading from an older version of Ansible-NAS, you will need to re-select your Time Machine back up disk by opening Time Machine Preferences and Selecting your backup disk via the \"Select Disk...\" option. Your Mac will find the old backups on the share and use them.","title":"Time Machine"},{"location":"applications/timemachine/#usage","text":"Set timemachine_enabled: true in your inventories/<your_inventory>/nas.yml file. Enabling Time Machine will result in the installation of Avahi on the NAS system (if it is not already installed) and a Time Machine service configuration file for Avahi will be added to the system (at /etc/avahi/services/timemachine.service ) to allow for Time Machine discovery by Macs on the local network. Avahi runs on the system, rather than in a container, as the same Avahi instance can be used to announce any number of services. The Samba server included in the Time Machine docker container logs to STDOUT and is compatible with Docker's built-in logging infrastructure.","title":"Usage"},{"location":"applications/timemachine/#specific-configuration","text":"timemachine_data_directory The absolute path on Ansible NAS where the backup files will be stored timemachine_volume_size_limit The maximum amount of space Time Machine can use for the backups in units of MiB. Set it to 0 for no limit. timemachine_share_name The name of the share as it will appear in the Time Machine application. Default is 'Data' timemachine_password The password used to access the share. Default is 'timemachine' ## Upgrading from AFP to SMB-based Time Machine Older versions of Time Machine included in Ansible-NAS relied on AFP (netatalk) shares. Apple has deprecated Time Machine over AFP in favor of SMB (Samba), and current versions of Ansible-NAS use a Samba-based Time Machine share. If you are upgrading from an older version of Ansible-NAS with the AFP-based Time Machine, you will need to re-select your Time Machine back up disk by opening Time Machine Preferences and Selecting your backup disk via the \"Select Disk...\" option. Your Mac will find the old backups on the share and use them.","title":"Specific Configuration"},{"location":"applications/traefik/","text":"Traefik Homepage: https://traefik.io Traefik is a reverse proxy used to provide external access to your Ansible-NAS box. Additionally, Traefik will automatically request and renew SSL certificates for you. You can configure which applications are available externally by enabling the <application_name>_available_externally setting for each application in the Advanced Settings section of your all.yml . See External Access for more info. Usage Set traefik_enabled: true in your inventories/<your_inventory>/nas.yml file. Traefik's web interface can be found at http://ansible_nas_host_or_ip:8083. Specific Configuration You'll need to map port 80 and 443 from your router to your Ansible-NAS box. A quick search should reveal instruction for your model of router.","title":"Traefik"},{"location":"applications/traefik/#traefik","text":"Homepage: https://traefik.io Traefik is a reverse proxy used to provide external access to your Ansible-NAS box. Additionally, Traefik will automatically request and renew SSL certificates for you. You can configure which applications are available externally by enabling the <application_name>_available_externally setting for each application in the Advanced Settings section of your all.yml . See External Access for more info.","title":"Traefik"},{"location":"applications/traefik/#usage","text":"Set traefik_enabled: true in your inventories/<your_inventory>/nas.yml file. Traefik's web interface can be found at http://ansible_nas_host_or_ip:8083.","title":"Usage"},{"location":"applications/traefik/#specific-configuration","text":"You'll need to map port 80 and 443 from your router to your Ansible-NAS box. A quick search should reveal instruction for your model of router.","title":"Specific Configuration"},{"location":"applications/transmission/","text":"Transmission Homepage: https://transmissionbt.com/ Transmission is a free BitTorrent client. Two versions are provided - one that tunnels through OpenVPN and one that connects directly. Usage Set transmission_enabled: true , or transmission_with_openvpn_enabled: true in your inventories/<your_inventory>/nas.yml file. Transmission's web interface can be found at http://ansible_nas_host_or_ip:9091 (with OpenVPN) or http://ansible_nas_host_or_ip:9092 (without OpenVPN). Specific Configuration If you enable Transmission with OpenVPN, you'll need to add the following to your inventory all.yml : openvpn_username: super_secret_username openvpn_password: super_secret_password openvpn_provider: NORDVPN openvpn_config: uk686.nordvpn.com.udp See https://hub.docker.com/r/haugene/transmission-openvpn/ for supported VPN providers.","title":"Transmission"},{"location":"applications/transmission/#transmission","text":"Homepage: https://transmissionbt.com/ Transmission is a free BitTorrent client. Two versions are provided - one that tunnels through OpenVPN and one that connects directly.","title":"Transmission"},{"location":"applications/transmission/#usage","text":"Set transmission_enabled: true , or transmission_with_openvpn_enabled: true in your inventories/<your_inventory>/nas.yml file. Transmission's web interface can be found at http://ansible_nas_host_or_ip:9091 (with OpenVPN) or http://ansible_nas_host_or_ip:9092 (without OpenVPN).","title":"Usage"},{"location":"applications/transmission/#specific-configuration","text":"If you enable Transmission with OpenVPN, you'll need to add the following to your inventory all.yml : openvpn_username: super_secret_username openvpn_password: super_secret_password openvpn_provider: NORDVPN openvpn_config: uk686.nordvpn.com.udp See https://hub.docker.com/r/haugene/transmission-openvpn/ for supported VPN providers.","title":"Specific Configuration"},{"location":"applications/ubooquity/","text":"Ubooquity Comic and Book Server Homepage: https://vaemendis.net/ubooquity/ Documentation: https://vaemendis.github.io/ubooquity-doc/ Docker Image: https://hub.docker.com/r/linuxserver/ubooquity/ Ubooquity is a free, lightweight and easy-to-use home server for your comics and ebooks. Use it to access your files from anywhere, with a tablet, an e-reader, a phone or a computer. Usage Set ubooquity_enabled: true in your inventories/<your_inventory>/nas.yml file. Access the webui at http:// :2202/ubooquity by default. See specific configuration section below for information on setting up external access. Specific Configuration Important note: if you want to access Ubooquity externally through Traefik (at ubooquity.yourdomain.tld), you need to go to http://ansible_nas_host_or_ip:2203/ubooquity/admin and set the reverse proxy prefix to blank under \"Advanced\". Otherwise you will need to append \"/ubooquity\" to the url in order to access. Admin login The admin portal is not exposed through Traefik. You can access the admin portal on port 2203. Upon your first run, the address is http://ansible_nas_host_or_ip:2203/ubooquity/admin. You will be able to set the admin password here.","title":"Ubooquity Comic and Book Server"},{"location":"applications/ubooquity/#ubooquity-comic-and-book-server","text":"Homepage: https://vaemendis.net/ubooquity/ Documentation: https://vaemendis.github.io/ubooquity-doc/ Docker Image: https://hub.docker.com/r/linuxserver/ubooquity/ Ubooquity is a free, lightweight and easy-to-use home server for your comics and ebooks. Use it to access your files from anywhere, with a tablet, an e-reader, a phone or a computer.","title":"Ubooquity Comic and Book Server"},{"location":"applications/ubooquity/#usage","text":"Set ubooquity_enabled: true in your inventories/<your_inventory>/nas.yml file. Access the webui at http:// :2202/ubooquity by default. See specific configuration section below for information on setting up external access.","title":"Usage"},{"location":"applications/ubooquity/#specific-configuration","text":"Important note: if you want to access Ubooquity externally through Traefik (at ubooquity.yourdomain.tld), you need to go to http://ansible_nas_host_or_ip:2203/ubooquity/admin and set the reverse proxy prefix to blank under \"Advanced\". Otherwise you will need to append \"/ubooquity\" to the url in order to access.","title":"Specific Configuration"},{"location":"applications/ubooquity/#admin-login","text":"The admin portal is not exposed through Traefik. You can access the admin portal on port 2203. Upon your first run, the address is http://ansible_nas_host_or_ip:2203/ubooquity/admin. You will be able to set the admin password here.","title":"Admin login"},{"location":"applications/utorrent/","text":"uTorrent Homepage: https://www.utorrent.com/ Docker Container: https://hub.docker.com/r/ekho/utorrent Usage Set utorrent_enabled: true in your inventories/<your_inventory>/nas.yml file. If you want to access uTorrent externally, don't forget to set utorrent_available_externally: \"true\" in your inventories/<your_inventory>/nas.yml file. The uTorrent web interface can be found at http://ansible_nas_host_or_ip:8111/gui: Username: admin Password:","title":"uTorrent"},{"location":"applications/utorrent/#utorrent","text":"Homepage: https://www.utorrent.com/ Docker Container: https://hub.docker.com/r/ekho/utorrent","title":"uTorrent"},{"location":"applications/utorrent/#usage","text":"Set utorrent_enabled: true in your inventories/<your_inventory>/nas.yml file. If you want to access uTorrent externally, don't forget to set utorrent_available_externally: \"true\" in your inventories/<your_inventory>/nas.yml file. The uTorrent web interface can be found at http://ansible_nas_host_or_ip:8111/gui: Username: admin Password:","title":"Usage"},{"location":"applications/virtual_desktop/","text":"Virtual Desktop It's possible to run a cut down desktop within a Docker container. We use RattyDAVE's custom Ubuntu Mate image . Usage Set virtual_desktop_enabled: true in your inventories/<your_inventory>/nas.yml file. Specific Configuration By default ansible_nas_user will be granted access with a password of topsecret with sudo rights. To change or add additional users override vd_users in your nas.yml : vd_users: - username: \"{{ ansible_nas_user }}\" password: \"topsecret\" sudo: \"Y\" - username: \"larrylaffer\" password: \"kensentme\" sudo: \"Y\" Mounts samba_shares_root is mounted to /samba . docker_home is mounted to /docker . Remote Access It's possible to access your virtual desktop through a web browser! Check out Guacamole .","title":"Virtual Desktop"},{"location":"applications/virtual_desktop/#virtual-desktop","text":"It's possible to run a cut down desktop within a Docker container. We use RattyDAVE's custom Ubuntu Mate image .","title":"Virtual Desktop"},{"location":"applications/virtual_desktop/#usage","text":"Set virtual_desktop_enabled: true in your inventories/<your_inventory>/nas.yml file.","title":"Usage"},{"location":"applications/virtual_desktop/#specific-configuration","text":"By default ansible_nas_user will be granted access with a password of topsecret with sudo rights. To change or add additional users override vd_users in your nas.yml : vd_users: - username: \"{{ ansible_nas_user }}\" password: \"topsecret\" sudo: \"Y\" - username: \"larrylaffer\" password: \"kensentme\" sudo: \"Y\"","title":"Specific Configuration"},{"location":"applications/virtual_desktop/#mounts","text":"samba_shares_root is mounted to /samba . docker_home is mounted to /docker .","title":"Mounts"},{"location":"applications/virtual_desktop/#remote-access","text":"It's possible to access your virtual desktop through a web browser! Check out Guacamole .","title":"Remote Access"},{"location":"applications/wallabag/","text":"wallabag Homepage: https://www.wallabag.org/ wallabag is a self-hostable PHP application allowing you to not miss any content anymore. Click, save and read it when you can. It extracts content so that you can read it when you have time. Usage Set wallabag_enabled: true in your inventories/<your_inventory>/nas.yml file. If you want to access wallabag externally, don't forget to set wallabag_available_externally: \"true\" in your inventories/<your_inventory>/nas.yml file. I recommend using the mobile app, which will sync with this installation so you have access to your saved articles even if you don't have signal or wifi access. The default credentials are wallabag:wallabag The wallabag web interface can be found at http://ansible_nas_host_or_ip:7780.","title":"wallabag"},{"location":"applications/wallabag/#wallabag","text":"Homepage: https://www.wallabag.org/ wallabag is a self-hostable PHP application allowing you to not miss any content anymore. Click, save and read it when you can. It extracts content so that you can read it when you have time.","title":"wallabag"},{"location":"applications/wallabag/#usage","text":"Set wallabag_enabled: true in your inventories/<your_inventory>/nas.yml file. If you want to access wallabag externally, don't forget to set wallabag_available_externally: \"true\" in your inventories/<your_inventory>/nas.yml file. I recommend using the mobile app, which will sync with this installation so you have access to your saved articles even if you don't have signal or wifi access. The default credentials are wallabag:wallabag The wallabag web interface can be found at http://ansible_nas_host_or_ip:7780.","title":"Usage"},{"location":"applications/watchtower/","text":"Watchtower Homepage: https://github.com/v2tec/watchtower A process for watching your Docker containers and automatically updating and restarting them whenever their base image is refreshed. Usage Set watchtower_enabled: true in your inventories/<your_inventory>/nas.yml file. Specific Configuration By default Watchtower is configured to check daily at 5am for updates. Various notification options are available, and can be configured by updating watchtower_command in your inventories/<your_inventory>/nas.yml file. A few examples are provided. The full set of options can be found at the Watchtower GitHub project page .","title":"Watchtower"},{"location":"applications/watchtower/#watchtower","text":"Homepage: https://github.com/v2tec/watchtower A process for watching your Docker containers and automatically updating and restarting them whenever their base image is refreshed.","title":"Watchtower"},{"location":"applications/watchtower/#usage","text":"Set watchtower_enabled: true in your inventories/<your_inventory>/nas.yml file.","title":"Usage"},{"location":"applications/watchtower/#specific-configuration","text":"By default Watchtower is configured to check daily at 5am for updates. Various notification options are available, and can be configured by updating watchtower_command in your inventories/<your_inventory>/nas.yml file. A few examples are provided. The full set of options can be found at the Watchtower GitHub project page .","title":"Specific Configuration"},{"location":"applications/youtubedlmaterial/","text":"YouTubeDL-Material Homepage: https://github.com/Tzahi12345/YoutubeDL-Material Docker Container: https://hub.docker.com/r/tzahi12345/youtubedl-material YoutubeDL-Material is a Material Design frontend for youtube-dl. It's coded using Angular 9 for the frontend, and Node.js on the backend. Usage Set youtubedlmaterial_enabled: true in your inventories/<your_inventory>/nas.yml file. The YouTubeDL-Material web interface can be found at http://ansible_nas_host_or_ip:8998. Specific Configuration A YouTube directory will be created in your configured downloads spot. YouTubeDL-Material downloads will be placed there. You can change the download location via inventories/<your_inventory>/nas.yml file.","title":"YouTubeDL-Material"},{"location":"applications/youtubedlmaterial/#youtubedl-material","text":"Homepage: https://github.com/Tzahi12345/YoutubeDL-Material Docker Container: https://hub.docker.com/r/tzahi12345/youtubedl-material YoutubeDL-Material is a Material Design frontend for youtube-dl. It's coded using Angular 9 for the frontend, and Node.js on the backend.","title":"YouTubeDL-Material"},{"location":"applications/youtubedlmaterial/#usage","text":"Set youtubedlmaterial_enabled: true in your inventories/<your_inventory>/nas.yml file. The YouTubeDL-Material web interface can be found at http://ansible_nas_host_or_ip:8998.","title":"Usage"},{"location":"applications/youtubedlmaterial/#specific-configuration","text":"A YouTube directory will be created in your configured downloads spot. YouTubeDL-Material downloads will be placed there. You can change the download location via inventories/<your_inventory>/nas.yml file.","title":"Specific Configuration"},{"location":"configuration/application_ports/","text":"Application Ports By default, applications can be found on the ports listed below. Application Port Notes Airsonic 4040 Bazarr 6767 Bitwarden \"hub\" 3012 Web Not. Bitwarden 19080 HTTP Calibre-web 8084 HTTP Cloud Commander 7373 Couchpotato 5050 Duplicati 8200 Emby 8096 HTTP Emby 8920 HTTPS Firefly III 8066 get_iplayer 8182 Gitea 3001 Web Gitea 222 SSH GitLab 4080 HTTP GitLab 4443 HTTPS GitLab 422 SSH Glances 61208 SSH Grafana 3000 Guacamole 8090 Heimdall 10080 Home Assistant 8123 Homebridge 8087 Jackett 9117 Jellyfin 8896 HTTP Jellyfin 8928 HTTPS Krusader 5800 HTTP Krusader 5900 VNC Lidarr 8686 MiniDLNA 8201 Miniflux 8070 Mosquitto 1883 MQTT Mosquitto 9001 Websocket Mylar 8585 HTTP MyMediaForAlexa 52051 Netdata 19999 Nextcloud 8080 NZBGet 6789 Ombi 3579 openHAB 7777 HTTP openHAB 7778 HTTPS Organizr 10081 HTTP Organizr 10444 HTTPS Plex 32400 Portainer 9000 pyload 8000 PyTivo 9032 HTTP PyTivo 2190 UDP Radarr 7878 Sickchill 8081 Sonarr 8989 Syncthing admin 8384 HTTP Syncthing P2P 22000 Tautulli 8185 The Lounge 9000 Time Machine 10445 SMB Traefik 8083 Transmission 9091 with VPN Transmission 3128 http proxy Transmission 9092 Ubooquity 2202 Ubooquity 2203 Admin uTorrent 8111 HTTP uTorrent 6881 BT uTorrent 6881 UDP Wallabag 7780 YouTubeDL-Mater 8998 HTTP ZNC 6677","title":"Application Ports"},{"location":"configuration/application_ports/#application-ports","text":"By default, applications can be found on the ports listed below. Application Port Notes Airsonic 4040 Bazarr 6767 Bitwarden \"hub\" 3012 Web Not. Bitwarden 19080 HTTP Calibre-web 8084 HTTP Cloud Commander 7373 Couchpotato 5050 Duplicati 8200 Emby 8096 HTTP Emby 8920 HTTPS Firefly III 8066 get_iplayer 8182 Gitea 3001 Web Gitea 222 SSH GitLab 4080 HTTP GitLab 4443 HTTPS GitLab 422 SSH Glances 61208 SSH Grafana 3000 Guacamole 8090 Heimdall 10080 Home Assistant 8123 Homebridge 8087 Jackett 9117 Jellyfin 8896 HTTP Jellyfin 8928 HTTPS Krusader 5800 HTTP Krusader 5900 VNC Lidarr 8686 MiniDLNA 8201 Miniflux 8070 Mosquitto 1883 MQTT Mosquitto 9001 Websocket Mylar 8585 HTTP MyMediaForAlexa 52051 Netdata 19999 Nextcloud 8080 NZBGet 6789 Ombi 3579 openHAB 7777 HTTP openHAB 7778 HTTPS Organizr 10081 HTTP Organizr 10444 HTTPS Plex 32400 Portainer 9000 pyload 8000 PyTivo 9032 HTTP PyTivo 2190 UDP Radarr 7878 Sickchill 8081 Sonarr 8989 Syncthing admin 8384 HTTP Syncthing P2P 22000 Tautulli 8185 The Lounge 9000 Time Machine 10445 SMB Traefik 8083 Transmission 9091 with VPN Transmission 3128 http proxy Transmission 9092 Ubooquity 2202 Ubooquity 2203 Admin uTorrent 8111 HTTP uTorrent 6881 BT uTorrent 6881 UDP Wallabag 7780 YouTubeDL-Mater 8998 HTTP ZNC 6677","title":"Application Ports"},{"location":"configuration/custom_applications/","text":"Custom Applications Using Portainer Ensure that you have portainer_enabled: true in your group_vars/all.yml file, and have run the playbook so that Portainer is up and running. Hit Portainer on http://ansible_nas_host_or_ip:9000. You can now deploy an 'App Template' or head to 'Containers' and manually enter container configuration. Using a Custom Ansible Task Needs to be docced","title":"Custom Applications"},{"location":"configuration/custom_applications/#custom-applications","text":"","title":"Custom Applications"},{"location":"configuration/custom_applications/#using-portainer","text":"Ensure that you have portainer_enabled: true in your group_vars/all.yml file, and have run the playbook so that Portainer is up and running. Hit Portainer on http://ansible_nas_host_or_ip:9000. You can now deploy an 'App Template' or head to 'Containers' and manually enter container configuration.","title":"Using Portainer"},{"location":"configuration/custom_applications/#using-a-custom-ansible-task","text":"Needs to be docced","title":"Using a Custom Ansible Task"},{"location":"configuration/external_access/","text":"External Access There are a number of steps required to enable external access to the applications running on your NAS: Enable Traefik Domain name and DNS configuration Router configuration Enable specific applications for external access :skull: :skull: :skull: Warning! :skull: :skull: :skull: Enabling access to applications externally does not automatically secure them. If you can access an application from within your own network without a username and password, this will also be the case externally. It is your responsibility to ensure that applications you enable external access to are secured appropriately! Enable Traefik Traefik routes traffic from ports 80 (HTTP) and 443 (HTTPS) on your Ansible-NAS box to the relevant application, based on hostname. Simply set traefik_enabled: true in your all.yml . By default it listens on ports 80 and 443, but doesn't route any traffic. Domain Name and DNS Configuration Set ansible_nas_domain to the domain name you want to use for your Ansible-NAS. You'll need somewhere to host the DNS for that domain - Cloudflare is a good free solution. Once you have an account and Cloudflare is hosting the DNS for your domain, create a wildcard DNS entry ( *.myawesomedomain.com ) and set it to your current IP address. You then need to enable the Cloudflare Dynamic DNS container ( cloudflare_ddns_enabled: true ) so the wildcard DNS entry for your domain name is updated if/when your ISP issues you a new IP address. Router Configuration You need to map ports 80 and 443 from your router to your Ansible-NAS box. How to do this is entirely dependent on your router (and out of scope of these docs), but if you're using Ansible-NAS then this should be within your skillset. :) Enable Specific Applications Every application has a <application_name>_available_externally setting in the Advanced Settings section of all.yml . Setting this to true will configure Traefik to route <application>.yourdomain.com to the application, making it available externally.","title":"External Access"},{"location":"configuration/external_access/#external-access","text":"There are a number of steps required to enable external access to the applications running on your NAS: Enable Traefik Domain name and DNS configuration Router configuration Enable specific applications for external access","title":"External Access"},{"location":"configuration/external_access/#skull-skull-skull-warning-skull-skull-skull","text":"Enabling access to applications externally does not automatically secure them. If you can access an application from within your own network without a username and password, this will also be the case externally. It is your responsibility to ensure that applications you enable external access to are secured appropriately!","title":":skull: :skull: :skull: Warning! :skull: :skull: :skull:"},{"location":"configuration/external_access/#enable-traefik","text":"Traefik routes traffic from ports 80 (HTTP) and 443 (HTTPS) on your Ansible-NAS box to the relevant application, based on hostname. Simply set traefik_enabled: true in your all.yml . By default it listens on ports 80 and 443, but doesn't route any traffic.","title":"Enable Traefik"},{"location":"configuration/external_access/#domain-name-and-dns-configuration","text":"Set ansible_nas_domain to the domain name you want to use for your Ansible-NAS. You'll need somewhere to host the DNS for that domain - Cloudflare is a good free solution. Once you have an account and Cloudflare is hosting the DNS for your domain, create a wildcard DNS entry ( *.myawesomedomain.com ) and set it to your current IP address. You then need to enable the Cloudflare Dynamic DNS container ( cloudflare_ddns_enabled: true ) so the wildcard DNS entry for your domain name is updated if/when your ISP issues you a new IP address.","title":"Domain Name and DNS Configuration"},{"location":"configuration/external_access/#router-configuration","text":"You need to map ports 80 and 443 from your router to your Ansible-NAS box. How to do this is entirely dependent on your router (and out of scope of these docs), but if you're using Ansible-NAS then this should be within your skillset. :)","title":"Router Configuration"},{"location":"configuration/external_access/#enable-specific-applications","text":"Every application has a <application_name>_available_externally setting in the Advanced Settings section of all.yml . Setting this to true will configure Traefik to route <application>.yourdomain.com to the application, making it available externally.","title":"Enable Specific Applications"},{"location":"configuration/nfs_exports/","text":"NFS Exports Ansible-NAS uses the awesome geerlingguy.nfs Ansible role to configure NFS exports. More info on configuring NFS exports can be found here . NFS Examples Ansible-NAS shares are defined in the nfs_exports section within group_vars/all.yml . The example provided will allow anyone to read the data in {{ nfs_shares_root }}/public on your Ansible-NAS box. Permissions NFS \"exports\" (an equivalent of a Samba share) are permissioned differently to Samba shares. Samba shares are permissioned with users and groups, and NFS exports are permissioned by the host wanting to access them, and then usual Linux permissions are applied to the files and directories within there. As mentioned above, the example will allow any computer on your network to read and write to the export.","title":"NFS Exports"},{"location":"configuration/nfs_exports/#nfs-exports","text":"Ansible-NAS uses the awesome geerlingguy.nfs Ansible role to configure NFS exports. More info on configuring NFS exports can be found here .","title":"NFS Exports"},{"location":"configuration/nfs_exports/#nfs-examples","text":"Ansible-NAS shares are defined in the nfs_exports section within group_vars/all.yml . The example provided will allow anyone to read the data in {{ nfs_shares_root }}/public on your Ansible-NAS box.","title":"NFS Examples"},{"location":"configuration/nfs_exports/#permissions","text":"NFS \"exports\" (an equivalent of a Samba share) are permissioned differently to Samba shares. Samba shares are permissioned with users and groups, and NFS exports are permissioned by the host wanting to access them, and then usual Linux permissions are applied to the files and directories within there. As mentioned above, the example will allow any computer on your network to read and write to the export.","title":"Permissions"},{"location":"configuration/samba_shares/","text":"Samba Shares Ansible-NAS uses the awesome bertvv.samba Ansible role to configure Samba - check out the project page for the many different options you can use to configure a share. Share Examples Ansible-NAS shares are defined in the samba_shares section within group_vars/all.yml . The examples provided are \"public\" shares that anyone on your LAN can read and write to. File Permissions Ansible-NAS creates an ansible-nas user and group on your server, which Samba will use to access the data in your shares. New data created will be permissioned correctly. However, if you have existing data this will need to be repermissioned so that Samba can read and serve it. An playbook is provided to do this for you - permission_data.yml . It is separated from the main Ansible-NAS playbook due to the time it can take to run with large amounts of data. You should only need to run this once.","title":"Samba Shares"},{"location":"configuration/samba_shares/#samba-shares","text":"Ansible-NAS uses the awesome bertvv.samba Ansible role to configure Samba - check out the project page for the many different options you can use to configure a share.","title":"Samba Shares"},{"location":"configuration/samba_shares/#share-examples","text":"Ansible-NAS shares are defined in the samba_shares section within group_vars/all.yml . The examples provided are \"public\" shares that anyone on your LAN can read and write to.","title":"Share Examples"},{"location":"configuration/samba_shares/#file-permissions","text":"Ansible-NAS creates an ansible-nas user and group on your server, which Samba will use to access the data in your shares. New data created will be permissioned correctly. However, if you have existing data this will need to be repermissioned so that Samba can read and serve it. An playbook is provided to do this for you - permission_data.yml . It is separated from the main Ansible-NAS playbook due to the time it can take to run with large amounts of data. You should only need to run this once.","title":"File Permissions"},{"location":"zfs/zfs_configuration/","text":"ZFS Configuration This text deals with specific ZFS configuration questions for Ansible-NAS. If you are new to ZFS and are looking for the big picture, please read the ZFS overview introduction first. Just so there is no misunderstanding Unlike other NAS variants, Ansible-NAS does not install, configure or manage the disks or file systems for you. It doesn't care which file system you use - ZFS, Btrfs, XFS or EXT4, take your pick. Nor does it provides a mechanism for snapshots or disk monitoring. As Tony Stark said to Loki in Avengers : It's all on you. However, Ansible-NAS has traditionally been used with the powerful ZFS filesystem. Since out of the box support for ZFS on Linux with Ubuntu is comparatively new, this text shows how to set up a simple storage configuration. To paraphrase Nick Fury from Winter Soldier : We do share. We're nice like that. Using ZFS for Docker containers is currently not covered by this document. See the official Docker ZFS documentation instead. The obligatory warning We take no responsibility for any bad thing that might happen if you follow this guide. We strongly suggest you test these procedures in a virtual machine first. Always, always, always backup your data. The basic setup For this example, we're assuming two identical spinning rust hard drives for Ansible-NAS storage. These two drives will be mirrored to provide redundancy. The actual Ubuntu system will be on a different drive and is not our concern. Root on ZFS is possible, but not something that has been tested with Ansible-NAS. The Ubuntu kernel is already ready for ZFS. We only need the utility package which we install with sudo apt install zfsutils . Creating a pool We assume you don't mind totally destroying whatever data might be on your two storage drives, have used a tool such as gparted to remove any existing partitions, and have installed a new GPT partition table on each drive. To create our ZFS pool, we will use a command in this form: sudo zpool create -o ashift=<ASHIFT> <NAME> mirror <DRIVE1> <DRIVE2> The options from simple to complex are: NAME : ZFS pools traditionally take their names from characters in the The Matrix . The two most common are tank and dozer . Whatever you use, it should be short - think ash , not xenomorph . DRIVES : The Linux command lsblk will give you a quick overview of the hard drives in the system. However, we don't pass the drive specification in the format /dev/sde because this is not persistent. Instead, always use the output of ls /dev/disk/by-id/ to find the drives' IDs. ASHIFT : This is required to pass the sector size of the drive to ZFS for optimal performance. You might have to do this by hand because some drives lie: Whereas modern drives have 4k sector sizes (or 8k for many SSDs), they will report 512 bytes because Windows XP can't handle 4k sectors . ZFS tries to catch the liars and use the correct value. However, this sometimes fails, and you have to add it by hand. The ashift value is a power of two, so we have 9 for 512 bytes, 12 for 4k, and 13 for 8k. You can create a pool without this parameter and then use zdb -C | grep ashift to see what ZFS generated automatically. If it isn't what you think, destroy the pool again and add it manually. In our pretend case, we use two 3 TB WD Red drives. Listing all drives by ID gives us something like this, but with real serial numbers: ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN01 ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN02 WD Reds have a 4k sector size. The actual command to create the pool would then be: sudo zpool create -o ashift=12 tank mirror ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN01 ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN02 Our new pool is named tank and is mirrored. To see information about it, use zpool status tank (no sudo necessary). If you screwed up (usually with ashift ), use sudo zpool destroy tank and start over now before it's too late. Pool and filesystem properties Pools have properties that apply either to the pool itself or to filesystems created in the pool. You can use the command zpool get all tank to see the pool properties and zfs get all tank to see the filesystem properties. Most default values are perfectly sensible, some you'll want to change . Setting defaults makes life easier when we create our filesystems. sudo zpool set autoexpand=on tank sudo zfs set atime=off tank sudo zfs set compression=lz4 tank autoexpand=on lets the pool grow when you add larger hard drives. atime=off means that your system won't update a time stamp every time a file is accessed, something which would use a lot of resources. Usually, you don't care. Compression is a no-brainer on modern CPUs and should be on by default (we will discuss exceptions for compressed media files later). Creating filesystems To actually store the data, we need filesystems (also known as \"datasets\"). For our very simple default Ansible-NAS setup, we will create two: One filesystem for movies ( movies_root in all.yml ) and one for downloads ( downloads_root ). Movies (and other large, pre-compressed files) We first create the basic filesystem: sudo zfs create tank/movies Movie files are usually rather large, already in a compressed format and for security reasons, the files stored there shouldn't be executable. We change the properties of the filesystem accordingly: sudo zfs set recordsize=1M tank/movies sudo zfs set compression=off tank/movies sudo zfs set exec=off tank/movies The recordsize here is set to the currently largest possible value to increase performance and save storage. Recall that we used ashift during the creation of the pool to match the ZFS block size with the drives' sector size. Records are created out of these blocks. Having larger records reduces the amount of metadata that is required, because various parts of ZFS such as caching and checksums work on this level. Compression is unnecessary for movie files because they are usually in a compressed format anyway. ZFS is good about recognizing this, and so if you happen to leave compression on as the default for the pool, it won't make much of a difference. By default , ZFS stores pools directly under the root directory. Also, the filesystems don't have to be listed in /etc/fstab to be mounted. This means that our filesystem will appear as /tank/movies if you don't change anything. We need to change the line in all.yml accordingly: movies_root: \"/tank/movies\" You can also set a traditional mount point if you wish with the mountpoint property. Setting this to none prevents the file system from being automatically mounted at all. The filesystems for TV shows, music files and podcasts - all large, pre-compressed files - should probably take the exact same parameters. Downloads For downloads, we can leave most of the default parameters the way they are. sudo zfs create tank/downloads sudo zfs set exec=off tank/downloads The recordsize stays the 128 KB default. In all.yml , the new line is downloads_root: \"/tank/downloads\" Other data Depending on the use case, you might want to create and tune more filesystems. For example, Bit Torrent , MySQL and Virtual Machines all have known best configurations. Setting up scrubs On Ubuntu, scrubs are configured out of the box to run on the second Sunday of every month. See /etc/cron.d/zfsutils-linux to change this. Email notifications To have the ZFS demon zed send you emails when there is trouble, you first have to install an email agent such as postfix. In the file /etc/zfs/zed.d/zed.rc , change the three entries: ZED_EMAIL_ADDR=<YOUR_EMAIL_ADDRESS_HERE> ZED_NOTIFY_INTERVAL_SECS=3600 ZED_NOTIFY_VERBOSE=1 If zed is not enabled, you might have to run systemctl enable zed . You can test the setup by manually starting a scrub with sudo zpool scrub tank . Snapshots Snapshots create a \"frozen\" version of a filesystem, providing a safe copy of the contents. Correctly configured, they provide good protection against accidental deletion and certain types of attacks such as ransomware. On copy-on-write (COW) filesystems such as ZFS, they are cheap and fast to create. It is very rare that you won't want snapshots. Snapshots do not replace the need for backups. Nothing replaces the need for backups except more backups. Managing snapshots by hand If you have data in a filesystem that never or very rarely changes, it might be easiest to just take a snapshot by hand after every major change. Use the zfs snapshot command with the name of the filesystem combined with an identifier separated by the @ sign. Traditionally, this somehow includes the date of the snapshot, usually in some variant of the ISO 8601 format. zfs snapshot tank/movies@2019-04-24 To see the list of snapshots in the system, run zfs list -t snapshot To revert (\"roll back\") to the previous snapshot, use the zfs rollback command. zfs rollback tank/movies@2019-04-24 By default, you can only roll back to the most recent snapshot. Anything before then requires trickery outside the scope of this document. Finally, to get rid of a snapshot, use the zfs destroy command. zfs destroy tank/movies@2019-04-24 Be very careful with destroy . If you leave out the snapshot identifier and only list the filesystem - in our example, tank/movies - the filesystem itself will immediately be destroyed. There will be no confirmation prompt, because ZFS doesn't believe in that sort of thing. Managing snapshots with Sanoid Usually, you'll want the process of creating new and deleting old snapshots to be automatic, especially on filesystems that change frequently. One tool for this is sanoid . There are various instructions for setting it up, the following is based on notes from SvennD . For this example, we'll assume we have a single dataset tank/movies that holds, ah, movies. First, we install sanoid to the /opt directory. This assumes that Perl itself is already installed. sudo apt install libconfig-inifiles-perl libcapture-tiny-perl cd /opt sudo git clone https://github.com/jimsalterjrs/sanoid It is probably easiest to link sanoid to /usr/sbin : sudo ln /opt/sanoid/sanoid /usr/sbin/ Then we need to setup the configuration files. sudo mkdir /etc/sanoid sudo cp /opt/sanoid/sanoid.conf /etc/sanoid/sanoid.conf sudo cp /opt/sanoid/sanoid.defaults.conf /etc/sanoid/sanoid.defaults.conf We don't change the defaults file, but it has to be copied to the folder anyway. Next, we edit the /etc/sanoid/sanoid.conf configuration file in two steps: We design the \"templates\" and then tell sanoid which filesystems to use it on. The configuration file included with sanoid contains a \"production\" template for filesystems that change frequently. For media files, we assume that there is not going to be that much change from day-to-day, and especially there will be very few deletions. We use snapshots because this provides protection against cryptolocker attacks and against accidental deletions. Again, snapshots, even lots of snapshots, do not replace backups. For our example, we configure for two hourly snapshots (against \"oh crap\" deletions), 31 daily, one monthly and one yearly snapshot. [template_media] frequently = 0 hourly = 2 daily = 31 monthly = 1 yearly = 1 autosnap = yes autoprune = yes That might seem like a bunch of daily snapshots, but remember, if nothing has changed, a ZFS snapshot is basically free. Once we have an entry for the template, we assign it to the filesystem. [tank/movies] use_template = media Finally, we edit /etc/crontab to run sanoid every five minutes: */5 * * * * root /usr/sbin/sanoid --cron After five minutes, you should see the first snapshots (use zfs list -t snapshot again). The list will look something like this mock example: NAME USED AVAIL REFER MOUNTPOINT tank/movies@autosnap_2019-05-17_13:55:01_yearly 0B - 1,53G - tank/movies@autosnap_2019-05-17_13:55:01_monthly 0B - 1,53G - tank/movies@autosnap_2019-05-17_13:55:01_daily 0B - 1,53G - Note that the snapshots use no storage, because we haven't changed anything. This is a very simple use of sanoid. Other functions include running scripts before and after snapshots, and setups to help with backups. See the included configuration files for examples.","title":"ZFS Configuration"},{"location":"zfs/zfs_configuration/#zfs-configuration","text":"This text deals with specific ZFS configuration questions for Ansible-NAS. If you are new to ZFS and are looking for the big picture, please read the ZFS overview introduction first.","title":"ZFS Configuration"},{"location":"zfs/zfs_configuration/#just-so-there-is-no-misunderstanding","text":"Unlike other NAS variants, Ansible-NAS does not install, configure or manage the disks or file systems for you. It doesn't care which file system you use - ZFS, Btrfs, XFS or EXT4, take your pick. Nor does it provides a mechanism for snapshots or disk monitoring. As Tony Stark said to Loki in Avengers : It's all on you. However, Ansible-NAS has traditionally been used with the powerful ZFS filesystem. Since out of the box support for ZFS on Linux with Ubuntu is comparatively new, this text shows how to set up a simple storage configuration. To paraphrase Nick Fury from Winter Soldier : We do share. We're nice like that. Using ZFS for Docker containers is currently not covered by this document. See the official Docker ZFS documentation instead.","title":"Just so there is no misunderstanding"},{"location":"zfs/zfs_configuration/#the-obligatory-warning","text":"We take no responsibility for any bad thing that might happen if you follow this guide. We strongly suggest you test these procedures in a virtual machine first. Always, always, always backup your data.","title":"The obligatory warning"},{"location":"zfs/zfs_configuration/#the-basic-setup","text":"For this example, we're assuming two identical spinning rust hard drives for Ansible-NAS storage. These two drives will be mirrored to provide redundancy. The actual Ubuntu system will be on a different drive and is not our concern. Root on ZFS is possible, but not something that has been tested with Ansible-NAS. The Ubuntu kernel is already ready for ZFS. We only need the utility package which we install with sudo apt install zfsutils .","title":"The basic setup"},{"location":"zfs/zfs_configuration/#creating-a-pool","text":"We assume you don't mind totally destroying whatever data might be on your two storage drives, have used a tool such as gparted to remove any existing partitions, and have installed a new GPT partition table on each drive. To create our ZFS pool, we will use a command in this form: sudo zpool create -o ashift=<ASHIFT> <NAME> mirror <DRIVE1> <DRIVE2> The options from simple to complex are: NAME : ZFS pools traditionally take their names from characters in the The Matrix . The two most common are tank and dozer . Whatever you use, it should be short - think ash , not xenomorph . DRIVES : The Linux command lsblk will give you a quick overview of the hard drives in the system. However, we don't pass the drive specification in the format /dev/sde because this is not persistent. Instead, always use the output of ls /dev/disk/by-id/ to find the drives' IDs. ASHIFT : This is required to pass the sector size of the drive to ZFS for optimal performance. You might have to do this by hand because some drives lie: Whereas modern drives have 4k sector sizes (or 8k for many SSDs), they will report 512 bytes because Windows XP can't handle 4k sectors . ZFS tries to catch the liars and use the correct value. However, this sometimes fails, and you have to add it by hand. The ashift value is a power of two, so we have 9 for 512 bytes, 12 for 4k, and 13 for 8k. You can create a pool without this parameter and then use zdb -C | grep ashift to see what ZFS generated automatically. If it isn't what you think, destroy the pool again and add it manually. In our pretend case, we use two 3 TB WD Red drives. Listing all drives by ID gives us something like this, but with real serial numbers: ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN01 ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN02 WD Reds have a 4k sector size. The actual command to create the pool would then be: sudo zpool create -o ashift=12 tank mirror ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN01 ata-WDC_WD30EFRX-68EUZN0_WD-WCCFAKESN02 Our new pool is named tank and is mirrored. To see information about it, use zpool status tank (no sudo necessary). If you screwed up (usually with ashift ), use sudo zpool destroy tank and start over now before it's too late.","title":"Creating a pool"},{"location":"zfs/zfs_configuration/#pool-and-filesystem-properties","text":"Pools have properties that apply either to the pool itself or to filesystems created in the pool. You can use the command zpool get all tank to see the pool properties and zfs get all tank to see the filesystem properties. Most default values are perfectly sensible, some you'll want to change . Setting defaults makes life easier when we create our filesystems. sudo zpool set autoexpand=on tank sudo zfs set atime=off tank sudo zfs set compression=lz4 tank autoexpand=on lets the pool grow when you add larger hard drives. atime=off means that your system won't update a time stamp every time a file is accessed, something which would use a lot of resources. Usually, you don't care. Compression is a no-brainer on modern CPUs and should be on by default (we will discuss exceptions for compressed media files later).","title":"Pool and filesystem properties"},{"location":"zfs/zfs_configuration/#creating-filesystems","text":"To actually store the data, we need filesystems (also known as \"datasets\"). For our very simple default Ansible-NAS setup, we will create two: One filesystem for movies ( movies_root in all.yml ) and one for downloads ( downloads_root ).","title":"Creating filesystems"},{"location":"zfs/zfs_configuration/#movies-and-other-large-pre-compressed-files","text":"We first create the basic filesystem: sudo zfs create tank/movies Movie files are usually rather large, already in a compressed format and for security reasons, the files stored there shouldn't be executable. We change the properties of the filesystem accordingly: sudo zfs set recordsize=1M tank/movies sudo zfs set compression=off tank/movies sudo zfs set exec=off tank/movies The recordsize here is set to the currently largest possible value to increase performance and save storage. Recall that we used ashift during the creation of the pool to match the ZFS block size with the drives' sector size. Records are created out of these blocks. Having larger records reduces the amount of metadata that is required, because various parts of ZFS such as caching and checksums work on this level. Compression is unnecessary for movie files because they are usually in a compressed format anyway. ZFS is good about recognizing this, and so if you happen to leave compression on as the default for the pool, it won't make much of a difference. By default , ZFS stores pools directly under the root directory. Also, the filesystems don't have to be listed in /etc/fstab to be mounted. This means that our filesystem will appear as /tank/movies if you don't change anything. We need to change the line in all.yml accordingly: movies_root: \"/tank/movies\" You can also set a traditional mount point if you wish with the mountpoint property. Setting this to none prevents the file system from being automatically mounted at all. The filesystems for TV shows, music files and podcasts - all large, pre-compressed files - should probably take the exact same parameters.","title":"Movies (and other large, pre-compressed files)"},{"location":"zfs/zfs_configuration/#downloads","text":"For downloads, we can leave most of the default parameters the way they are. sudo zfs create tank/downloads sudo zfs set exec=off tank/downloads The recordsize stays the 128 KB default. In all.yml , the new line is downloads_root: \"/tank/downloads\"","title":"Downloads"},{"location":"zfs/zfs_configuration/#other-data","text":"Depending on the use case, you might want to create and tune more filesystems. For example, Bit Torrent , MySQL and Virtual Machines all have known best configurations.","title":"Other data"},{"location":"zfs/zfs_configuration/#setting-up-scrubs","text":"On Ubuntu, scrubs are configured out of the box to run on the second Sunday of every month. See /etc/cron.d/zfsutils-linux to change this.","title":"Setting up scrubs"},{"location":"zfs/zfs_configuration/#email-notifications","text":"To have the ZFS demon zed send you emails when there is trouble, you first have to install an email agent such as postfix. In the file /etc/zfs/zed.d/zed.rc , change the three entries: ZED_EMAIL_ADDR=<YOUR_EMAIL_ADDRESS_HERE> ZED_NOTIFY_INTERVAL_SECS=3600 ZED_NOTIFY_VERBOSE=1 If zed is not enabled, you might have to run systemctl enable zed . You can test the setup by manually starting a scrub with sudo zpool scrub tank .","title":"Email notifications"},{"location":"zfs/zfs_configuration/#snapshots","text":"Snapshots create a \"frozen\" version of a filesystem, providing a safe copy of the contents. Correctly configured, they provide good protection against accidental deletion and certain types of attacks such as ransomware. On copy-on-write (COW) filesystems such as ZFS, they are cheap and fast to create. It is very rare that you won't want snapshots. Snapshots do not replace the need for backups. Nothing replaces the need for backups except more backups.","title":"Snapshots"},{"location":"zfs/zfs_configuration/#managing-snapshots-by-hand","text":"If you have data in a filesystem that never or very rarely changes, it might be easiest to just take a snapshot by hand after every major change. Use the zfs snapshot command with the name of the filesystem combined with an identifier separated by the @ sign. Traditionally, this somehow includes the date of the snapshot, usually in some variant of the ISO 8601 format. zfs snapshot tank/movies@2019-04-24 To see the list of snapshots in the system, run zfs list -t snapshot To revert (\"roll back\") to the previous snapshot, use the zfs rollback command. zfs rollback tank/movies@2019-04-24 By default, you can only roll back to the most recent snapshot. Anything before then requires trickery outside the scope of this document. Finally, to get rid of a snapshot, use the zfs destroy command. zfs destroy tank/movies@2019-04-24 Be very careful with destroy . If you leave out the snapshot identifier and only list the filesystem - in our example, tank/movies - the filesystem itself will immediately be destroyed. There will be no confirmation prompt, because ZFS doesn't believe in that sort of thing.","title":"Managing snapshots by hand"},{"location":"zfs/zfs_configuration/#managing-snapshots-with-sanoid","text":"Usually, you'll want the process of creating new and deleting old snapshots to be automatic, especially on filesystems that change frequently. One tool for this is sanoid . There are various instructions for setting it up, the following is based on notes from SvennD . For this example, we'll assume we have a single dataset tank/movies that holds, ah, movies. First, we install sanoid to the /opt directory. This assumes that Perl itself is already installed. sudo apt install libconfig-inifiles-perl libcapture-tiny-perl cd /opt sudo git clone https://github.com/jimsalterjrs/sanoid It is probably easiest to link sanoid to /usr/sbin : sudo ln /opt/sanoid/sanoid /usr/sbin/ Then we need to setup the configuration files. sudo mkdir /etc/sanoid sudo cp /opt/sanoid/sanoid.conf /etc/sanoid/sanoid.conf sudo cp /opt/sanoid/sanoid.defaults.conf /etc/sanoid/sanoid.defaults.conf We don't change the defaults file, but it has to be copied to the folder anyway. Next, we edit the /etc/sanoid/sanoid.conf configuration file in two steps: We design the \"templates\" and then tell sanoid which filesystems to use it on. The configuration file included with sanoid contains a \"production\" template for filesystems that change frequently. For media files, we assume that there is not going to be that much change from day-to-day, and especially there will be very few deletions. We use snapshots because this provides protection against cryptolocker attacks and against accidental deletions. Again, snapshots, even lots of snapshots, do not replace backups. For our example, we configure for two hourly snapshots (against \"oh crap\" deletions), 31 daily, one monthly and one yearly snapshot. [template_media] frequently = 0 hourly = 2 daily = 31 monthly = 1 yearly = 1 autosnap = yes autoprune = yes That might seem like a bunch of daily snapshots, but remember, if nothing has changed, a ZFS snapshot is basically free. Once we have an entry for the template, we assign it to the filesystem. [tank/movies] use_template = media Finally, we edit /etc/crontab to run sanoid every five minutes: */5 * * * * root /usr/sbin/sanoid --cron After five minutes, you should see the first snapshots (use zfs list -t snapshot again). The list will look something like this mock example: NAME USED AVAIL REFER MOUNTPOINT tank/movies@autosnap_2019-05-17_13:55:01_yearly 0B - 1,53G - tank/movies@autosnap_2019-05-17_13:55:01_monthly 0B - 1,53G - tank/movies@autosnap_2019-05-17_13:55:01_daily 0B - 1,53G - Note that the snapshots use no storage, because we haven't changed anything. This is a very simple use of sanoid. Other functions include running scripts before and after snapshots, and setups to help with backups. See the included configuration files for examples.","title":"Managing snapshots with Sanoid"},{"location":"zfs/zfs_overview/","text":"ZFS Overview This is a general overview of the ZFS file system for people who are new to it. If you have some experience and are actually looking for specific information about how to configure ZFS for Ansible-NAS, check out the ZFS example configuration . What is ZFS and why would I want it? ZFS is an advanced filesystem and volume manager originally created by Sun Microsystems starting in 2001. First released in 2005 for OpenSolaris, Oracle later bought Sun and switched to developing ZFS as closed source software. An open source fork took the name OpenZFS , but is still called \"ZFS\" for short. It runs on Linux, FreeBSD, illumos and other platforms. ZFS aims to be the \"last word in filesystems\" , a technology so future-proof that Michael W. Lucas and Allan Jude famously stated that the Enterprise's computer on Star Trek probably runs it. The design was based on four principles : \"Pooled\" storage to eliminate the notion of volumes. You can add more storage the same way you just add a RAM stick to memory. Make sure data is always consistent on the disks. There is no fsck command for ZFS and none is needed. Detect and correct data corruption (\"bitrot\"). ZFS is one of the few storage systems that checksums everything, including the data itself, and is \"self-healing\". Make it easy to use. Try to \"end the suffering\" for the admins involved in managing storage. ZFS includes a host of other features such as snapshots, transparent compression and encryption. During the early years of ZFS, this all came with hardware requirements only enterprise users could afford. By now, however, computers have become so powerful that ZFS can run (with some effort) on a Raspberry Pi . FreeBSD and FreeNAS make extensive use of ZFS. What is holding ZFS back on Linux are licensing issues beyond the scope of this document. Ansible-NAS doesn't actually specify a filesystem - you can use EXT4, XFS or Btrfs as well. However, ZFS not only provides the benefits listed above, but also lets you use your hard drives with different operating systems. Some people now using Ansible-NAS came from FreeNAS, and were able to export their ZFS storage drives there and import them to Ubuntu. On the other hand, if you ever decide to switch back to FreeNAS or maybe want to use FreeBSD instead of Linux, you should be able to use the same ZFS pools. An overview and some actual commands Storage in ZFS is organized in pools . Inside these pools, you create filesystems (also known as \"datasets\") which are like partitions on steroids. For instance, you can keep each user's /home directory in a separate filesystem. ZFS systems tend to use lots and lots of specialized filesystems with tailored parameters such as record size and compression. All filesystems share the available storage in their pool. Pools do not directly consist of hard disks or SSDs. Instead, drives are organized as virtual devices (VDEVs). This is where the physical redundancy in ZFS is located. Drives in a VDEV can be \"mirrored\" or combined as \"RaidZ\", roughly the equivalent of RAID5. These VDEVs are then combined into a pool by the administrator. The command might look something like this: sudo zpool create tank mirror /dev/sda /dev/sdb This combines /dev/sba and /dev/sdb to a mirrored VDEV, and then defines a new pool named tank consisting of this single VDEV. (Actually, you'd want to use a different ID for the drives, but you get the idea.) You can now create a filesystem in this pool for, say, all of your Mass Effect fan fiction: sudo zfs create tank/mefanfic You can then enable automatic compression on this filesystem with sudo zfs set compression=lz4 tank/mefanfic . To take a snapshot , use sudo zfs snapshot tank/mefanfic@21540411 Now, if evil people were somehow able to encrypt your precious fan fiction files with ransomware, you can simply laugh maniacally and revert to the old version: sudo zfs rollback tank/mefanfic@21540411 Of course, you would lose any texts you might have added to the filesystem between that snapshot and now. Usually, you'll have some form of automatic snapshot administration configured. To detect bitrot and other data defects, ZFS periodically runs scrubs : The system compares the available copies of each data record with their checksums. If there is a mismatch, the data is repaired. Known issues At time of writing (April 2019), ZFS on Linux does not offer native encryption, TRIM support or device removal, which are all scheduled to be included in the upcoming 0.8 release any day now. ZFS' original design for enterprise systems and redundancy requirements can make some things difficult. You can't just add individual drives to a pool and tell the system to reconfigure automatically. Instead, you have to either add a new VDEV, or replace each of the existing drives with one of higher capacity. In an enterprise environment, of course, you would just buy a bunch of new drives and move the data from the old pool to the new pool. Shrinking a pool is even harder - put simply, ZFS is not built for this, though it is being worked on . If you absolutely must be able to add or remove single drives, ZFS might not be the filesystem for you. Myths and misunderstandings Information on the internet about ZFS can be outdated, conflicting or flat-out wrong. Partially this is because it has been in use for almost 15 years now and things change, partially it is the result of being used on different operating systems which have minor differences under the hood. Also, Google searches tend to first return the Oracle documentation for their closed source ZFS variant, which is increasingly diverging from the open source OpenZFS standard. To clear up some of the most common misunderstandings: No, ZFS does not need at least 8 GB of RAM This myth is especially common in FreeNAS circles . Curiously, FreeBSD, the basis of FreeNAS, will run with 1 GB . The ZFS on Linux FAQ , which is more relevant for Ansible-NAS, states under \"suggested hardware\": 8GB+ of memory for the best performance. It's perfectly possible to run with 2GB or less (and people do), but you'll need more if using deduplication. (Deduplication is only useful in special cases . If you are reading this, you probably don't need it.) Experience shows that 8 GB of RAM is in fact a sensible minimal amount for continuous use. But it's not a requirement. What everybody agrees on is that ZFS loves RAM and works better the more it has, so you should have as much of it as you possibly can. When in doubt, add more RAM, and even more, and them some, until your motherboard's capacity is reached. No, ECC RAM is not required for ZFS This is another case where a recommendation has been taken as a requirement. To quote the ZFS on Linux FAQ again: Using ECC memory for OpenZFS is strongly recommended for enterprise environments where the strongest data integrity guarantees are required. Without ECC memory rare random bit flips caused by cosmic rays or by faulty memory can go undetected. If this were to occur OpenZFS (or any other filesystem) will write the damaged data to disk and be unable to automatically detect the corruption. ECC corrects single bit errors in memory. It is always better to have it on any computer if you can afford it, and ZFS is no exception. However, there is absolutely no requirement for ZFS to have ECC RAM. If you just don't care about the danger of random bit flips because, hey, you can always just download Night of the Living Dead all over again, you're perfectly free to use normal RAM. If you do use ECC RAM, make sure your processor and motherboard support it. No, the SLOG is not really a write cache You'll read the suggestion to add a fast SSD or NVMe as a \"SLOG drive\" (mistakenly also called \"ZIL\") for write caching. This isn't what happens, because ZFS already includes a write cache in RAM. Since RAM is always faster, adding a disk as a write cache doesn't even make sense. What the ZFS Intent Log (ZIL) does, with or without a dedicated drive, is handle synchronous writes. These occur when the system refuses to signal a successful write until the data is actually stored on a physical disk somewhere. This keeps the data safe, but is slower. By default, the ZIL initially shoves a copy of the data on a normal VDEV somewhere and then gives the thumbs up. The actual write to the pool is performed later from the write cache in RAM, not the temporary copy. The data there is only ever read if the power fails before the last step. The ZIL is all about protecting data, not making transfers faster. A Separate Intent Log (SLOG) is an additional fast drive for these temporary synchronous writes. It simply allows the ZIL give the thumbs up quicker. This means that a SLOG is never read unless the power has failed before the final write to the pool. Asynchronous writes just go through the normal write cache, by the way. If the power fails, the data is gone. In summary, the ZIL prevents data loss during synchronous writes, or at least ensures that the data in storage is consistent. You always have a ZIL. A SLOG will make the ZIL faster. You'll probably need to do some research and some testing to figure out if your system would benefit from a SLOG. NFS for instance uses synchronous writes, SMB usually doesn't. When in doubt, add more RAM instead. Further reading and viewing In 2012, Aaron Toponce wrote a now slightly dated, but still very good introduction to ZFS on Linux. If you only read one part, make it the explanation of the ARC , ZFS' read cache. One of the best books on ZFS around is FreeBSD Mastery: ZFS by Michael W. Lucas and Allan Jude. Though it is written for FreeBSD, the general guidelines apply for all variants. There is a second volume for advanced use. Jeff Bonwick, one of the original creators of ZFS, tells the story of how ZFS came to be on YouTube .","title":"ZFS Overview"},{"location":"zfs/zfs_overview/#zfs-overview","text":"This is a general overview of the ZFS file system for people who are new to it. If you have some experience and are actually looking for specific information about how to configure ZFS for Ansible-NAS, check out the ZFS example configuration .","title":"ZFS Overview"},{"location":"zfs/zfs_overview/#what-is-zfs-and-why-would-i-want-it","text":"ZFS is an advanced filesystem and volume manager originally created by Sun Microsystems starting in 2001. First released in 2005 for OpenSolaris, Oracle later bought Sun and switched to developing ZFS as closed source software. An open source fork took the name OpenZFS , but is still called \"ZFS\" for short. It runs on Linux, FreeBSD, illumos and other platforms. ZFS aims to be the \"last word in filesystems\" , a technology so future-proof that Michael W. Lucas and Allan Jude famously stated that the Enterprise's computer on Star Trek probably runs it. The design was based on four principles : \"Pooled\" storage to eliminate the notion of volumes. You can add more storage the same way you just add a RAM stick to memory. Make sure data is always consistent on the disks. There is no fsck command for ZFS and none is needed. Detect and correct data corruption (\"bitrot\"). ZFS is one of the few storage systems that checksums everything, including the data itself, and is \"self-healing\". Make it easy to use. Try to \"end the suffering\" for the admins involved in managing storage. ZFS includes a host of other features such as snapshots, transparent compression and encryption. During the early years of ZFS, this all came with hardware requirements only enterprise users could afford. By now, however, computers have become so powerful that ZFS can run (with some effort) on a Raspberry Pi . FreeBSD and FreeNAS make extensive use of ZFS. What is holding ZFS back on Linux are licensing issues beyond the scope of this document. Ansible-NAS doesn't actually specify a filesystem - you can use EXT4, XFS or Btrfs as well. However, ZFS not only provides the benefits listed above, but also lets you use your hard drives with different operating systems. Some people now using Ansible-NAS came from FreeNAS, and were able to export their ZFS storage drives there and import them to Ubuntu. On the other hand, if you ever decide to switch back to FreeNAS or maybe want to use FreeBSD instead of Linux, you should be able to use the same ZFS pools.","title":"What is ZFS and why would I want it?"},{"location":"zfs/zfs_overview/#an-overview-and-some-actual-commands","text":"Storage in ZFS is organized in pools . Inside these pools, you create filesystems (also known as \"datasets\") which are like partitions on steroids. For instance, you can keep each user's /home directory in a separate filesystem. ZFS systems tend to use lots and lots of specialized filesystems with tailored parameters such as record size and compression. All filesystems share the available storage in their pool. Pools do not directly consist of hard disks or SSDs. Instead, drives are organized as virtual devices (VDEVs). This is where the physical redundancy in ZFS is located. Drives in a VDEV can be \"mirrored\" or combined as \"RaidZ\", roughly the equivalent of RAID5. These VDEVs are then combined into a pool by the administrator. The command might look something like this: sudo zpool create tank mirror /dev/sda /dev/sdb This combines /dev/sba and /dev/sdb to a mirrored VDEV, and then defines a new pool named tank consisting of this single VDEV. (Actually, you'd want to use a different ID for the drives, but you get the idea.) You can now create a filesystem in this pool for, say, all of your Mass Effect fan fiction: sudo zfs create tank/mefanfic You can then enable automatic compression on this filesystem with sudo zfs set compression=lz4 tank/mefanfic . To take a snapshot , use sudo zfs snapshot tank/mefanfic@21540411 Now, if evil people were somehow able to encrypt your precious fan fiction files with ransomware, you can simply laugh maniacally and revert to the old version: sudo zfs rollback tank/mefanfic@21540411 Of course, you would lose any texts you might have added to the filesystem between that snapshot and now. Usually, you'll have some form of automatic snapshot administration configured. To detect bitrot and other data defects, ZFS periodically runs scrubs : The system compares the available copies of each data record with their checksums. If there is a mismatch, the data is repaired.","title":"An overview and some actual commands"},{"location":"zfs/zfs_overview/#known-issues","text":"At time of writing (April 2019), ZFS on Linux does not offer native encryption, TRIM support or device removal, which are all scheduled to be included in the upcoming 0.8 release any day now. ZFS' original design for enterprise systems and redundancy requirements can make some things difficult. You can't just add individual drives to a pool and tell the system to reconfigure automatically. Instead, you have to either add a new VDEV, or replace each of the existing drives with one of higher capacity. In an enterprise environment, of course, you would just buy a bunch of new drives and move the data from the old pool to the new pool. Shrinking a pool is even harder - put simply, ZFS is not built for this, though it is being worked on . If you absolutely must be able to add or remove single drives, ZFS might not be the filesystem for you.","title":"Known issues"},{"location":"zfs/zfs_overview/#myths-and-misunderstandings","text":"Information on the internet about ZFS can be outdated, conflicting or flat-out wrong. Partially this is because it has been in use for almost 15 years now and things change, partially it is the result of being used on different operating systems which have minor differences under the hood. Also, Google searches tend to first return the Oracle documentation for their closed source ZFS variant, which is increasingly diverging from the open source OpenZFS standard. To clear up some of the most common misunderstandings:","title":"Myths and misunderstandings"},{"location":"zfs/zfs_overview/#no-zfs-does-not-need-at-least-8-gb-of-ram","text":"This myth is especially common in FreeNAS circles . Curiously, FreeBSD, the basis of FreeNAS, will run with 1 GB . The ZFS on Linux FAQ , which is more relevant for Ansible-NAS, states under \"suggested hardware\": 8GB+ of memory for the best performance. It's perfectly possible to run with 2GB or less (and people do), but you'll need more if using deduplication. (Deduplication is only useful in special cases . If you are reading this, you probably don't need it.) Experience shows that 8 GB of RAM is in fact a sensible minimal amount for continuous use. But it's not a requirement. What everybody agrees on is that ZFS loves RAM and works better the more it has, so you should have as much of it as you possibly can. When in doubt, add more RAM, and even more, and them some, until your motherboard's capacity is reached.","title":"No, ZFS does not need at least 8 GB of RAM"},{"location":"zfs/zfs_overview/#no-ecc-ram-is-not-required-for-zfs","text":"This is another case where a recommendation has been taken as a requirement. To quote the ZFS on Linux FAQ again: Using ECC memory for OpenZFS is strongly recommended for enterprise environments where the strongest data integrity guarantees are required. Without ECC memory rare random bit flips caused by cosmic rays or by faulty memory can go undetected. If this were to occur OpenZFS (or any other filesystem) will write the damaged data to disk and be unable to automatically detect the corruption. ECC corrects single bit errors in memory. It is always better to have it on any computer if you can afford it, and ZFS is no exception. However, there is absolutely no requirement for ZFS to have ECC RAM. If you just don't care about the danger of random bit flips because, hey, you can always just download Night of the Living Dead all over again, you're perfectly free to use normal RAM. If you do use ECC RAM, make sure your processor and motherboard support it.","title":"No, ECC RAM is not required for ZFS"},{"location":"zfs/zfs_overview/#no-the-slog-is-not-really-a-write-cache","text":"You'll read the suggestion to add a fast SSD or NVMe as a \"SLOG drive\" (mistakenly also called \"ZIL\") for write caching. This isn't what happens, because ZFS already includes a write cache in RAM. Since RAM is always faster, adding a disk as a write cache doesn't even make sense. What the ZFS Intent Log (ZIL) does, with or without a dedicated drive, is handle synchronous writes. These occur when the system refuses to signal a successful write until the data is actually stored on a physical disk somewhere. This keeps the data safe, but is slower. By default, the ZIL initially shoves a copy of the data on a normal VDEV somewhere and then gives the thumbs up. The actual write to the pool is performed later from the write cache in RAM, not the temporary copy. The data there is only ever read if the power fails before the last step. The ZIL is all about protecting data, not making transfers faster. A Separate Intent Log (SLOG) is an additional fast drive for these temporary synchronous writes. It simply allows the ZIL give the thumbs up quicker. This means that a SLOG is never read unless the power has failed before the final write to the pool. Asynchronous writes just go through the normal write cache, by the way. If the power fails, the data is gone. In summary, the ZIL prevents data loss during synchronous writes, or at least ensures that the data in storage is consistent. You always have a ZIL. A SLOG will make the ZIL faster. You'll probably need to do some research and some testing to figure out if your system would benefit from a SLOG. NFS for instance uses synchronous writes, SMB usually doesn't. When in doubt, add more RAM instead.","title":"No, the SLOG is not really a write cache"},{"location":"zfs/zfs_overview/#further-reading-and-viewing","text":"In 2012, Aaron Toponce wrote a now slightly dated, but still very good introduction to ZFS on Linux. If you only read one part, make it the explanation of the ARC , ZFS' read cache. One of the best books on ZFS around is FreeBSD Mastery: ZFS by Michael W. Lucas and Allan Jude. Though it is written for FreeBSD, the general guidelines apply for all variants. There is a second volume for advanced use. Jeff Bonwick, one of the original creators of ZFS, tells the story of how ZFS came to be on YouTube .","title":"Further reading and viewing"}]}